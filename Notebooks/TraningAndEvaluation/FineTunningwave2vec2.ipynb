{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 2,
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   "id": "783b1ff5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
<<<<<<< HEAD
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchaudio\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset, Dataset\n",
=======
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchaudio\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset, Dataset\n",
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\__init__.py:59\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[32m     61\u001b[39m     ArrowDtype,\n\u001b[32m     62\u001b[39m     Int8Dtype,\n\u001b[32m     63\u001b[39m     Int16Dtype,\n\u001b[32m     64\u001b[39m     Int32Dtype,\n\u001b[32m     65\u001b[39m     Int64Dtype,\n\u001b[32m     66\u001b[39m     UInt8Dtype,\n\u001b[32m     67\u001b[39m     UInt16Dtype,\n\u001b[32m     68\u001b[39m     UInt32Dtype,\n\u001b[32m     69\u001b[39m     UInt64Dtype,\n\u001b[32m     70\u001b[39m     Float32Dtype,\n\u001b[32m     71\u001b[39m     Float64Dtype,\n\u001b[32m     72\u001b[39m     CategoricalDtype,\n\u001b[32m     73\u001b[39m     PeriodDtype,\n\u001b[32m     74\u001b[39m     IntervalDtype,\n\u001b[32m     75\u001b[39m     DatetimeTZDtype,\n\u001b[32m     76\u001b[39m     StringDtype,\n\u001b[32m     77\u001b[39m     BooleanDtype,\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[32m     79\u001b[39m     NA,\n\u001b[32m     80\u001b[39m     isna,\n\u001b[32m     81\u001b[39m     isnull,\n\u001b[32m     82\u001b[39m     notna,\n\u001b[32m     83\u001b[39m     notnull,\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[32m     85\u001b[39m     Index,\n\u001b[32m     86\u001b[39m     CategoricalIndex,\n\u001b[32m     87\u001b[39m     RangeIndex,\n\u001b[32m     88\u001b[39m     MultiIndex,\n\u001b[32m     89\u001b[39m     IntervalIndex,\n\u001b[32m     90\u001b[39m     TimedeltaIndex,\n\u001b[32m     91\u001b[39m     DatetimeIndex,\n\u001b[32m     92\u001b[39m     PeriodIndex,\n\u001b[32m     93\u001b[39m     IndexSlice,\n\u001b[32m     94\u001b[39m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[32m     95\u001b[39m     NaT,\n\u001b[32m     96\u001b[39m     Period,\n\u001b[32m     97\u001b[39m     period_range,\n\u001b[32m     98\u001b[39m     Timedelta,\n\u001b[32m     99\u001b[39m     timedelta_range,\n\u001b[32m    100\u001b[39m     Timestamp,\n\u001b[32m    101\u001b[39m     date_range,\n\u001b[32m    102\u001b[39m     bdate_range,\n\u001b[32m    103\u001b[39m     Interval,\n\u001b[32m    104\u001b[39m     interval_range,\n\u001b[32m    105\u001b[39m     DateOffset,\n\u001b[32m    106\u001b[39m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[32m    107\u001b[39m     to_numeric,\n\u001b[32m    108\u001b[39m     to_datetime,\n\u001b[32m    109\u001b[39m     to_timedelta,\n\u001b[32m    110\u001b[39m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[32m    111\u001b[39m     Flags,\n\u001b[32m    112\u001b[39m     Grouper,\n\u001b[32m    113\u001b[39m     factorize,\n\u001b[32m    114\u001b[39m     unique,\n\u001b[32m    115\u001b[39m     value_counts,\n\u001b[32m    116\u001b[39m     NamedAgg,\n\u001b[32m    117\u001b[39m     array,\n\u001b[32m    118\u001b[39m     Categorical,\n\u001b[32m    119\u001b[39m     set_eng_float_format,\n\u001b[32m    120\u001b[39m     Series,\n\u001b[32m    121\u001b[39m     DataFrame,\n\u001b[32m    122\u001b[39m )\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtseries\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     NaT,\n\u001b[32m      3\u001b[39m     Period,\n\u001b[32m      4\u001b[39m     Timedelta,\n\u001b[32m      5\u001b[39m     Timestamp,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     ArrowDtype,\n\u001b[32m     11\u001b[39m     CategoricalDtype,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     PeriodDtype,\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\__init__.py:18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401,E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minterval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     NaT,\n\u001b[32m     21\u001b[39m     NaTType,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     iNaT,\n\u001b[32m     27\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32minterval.pyx:1\u001b[39m, in \u001b[36minit pandas._libs.interval\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
<<<<<<< HEAD
    "from datasets import Dataset\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification,DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "import gc\n",
    "import numpy as np\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
=======
    "from datasets import load_dataset, Dataset\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification, DataCollatorWithPadding\n"
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 19,
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   "id": "07fd1dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "  Emotions                                               Path\n",
      "0  neutral  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...\n",
      "1  neutral  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...\n",
      "2  neutral  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...\n",
      "3  neutral  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...\n",
      "4     calm  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...\n",
      "\n",
      "Column names: ['Emotions', 'Path']\n",
      "\n",
      "DataFrame shape before cleaning: (12161, 2)\n",
      "DataFrame shape after removing nulls: (12161, 2)\n",
      "Removed 0 rows with non-existent files\n",
      "\n",
      "Found 8 unique labels:\n",
      "  angry: 0\n",
      "  calm: 1\n",
      "  disgust: 2\n",
      "  fear: 3\n",
      "  happy: 4\n",
      "  neutral: 5\n",
      "  sad: 6\n",
      "  surprise: 7\n",
      "\n",
      "DataFrame after adding label_id:\n",
      "                                                path    label  label_id\n",
      "0  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...  neutral         5\n",
      "1  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...  neutral         5\n",
      "2  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...  neutral         5\n",
      "3  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...  neutral         5\n",
      "4  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...     calm         1\n",
      "\n",
      "Dataset created successfully!\n",
      "Dataset info: Dataset({\n",
      "    features: ['label', 'path', 'label_id'],\n",
      "    num_rows: 12161\n",
      "})\n",
      "Processor loaded successfully!\n",
      "\n",
      "Final dataset shape: 12161 samples\n",
      "Features: {'label': Value(dtype='string', id=None), 'path': Value(dtype='string', id=None), 'label_id': Value(dtype='int64', id=None)}\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "MODEL_NAME = \"facebook/wav2vec2-base\"\n",
    "NUM_LABELS = 8  # Change this if needed\n",
    "\n",
    "# ===============================\n",
    "# 2. Load Processor and Model\n",
    "# ===============================\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"single_label_classification\"\n",
    ").to(device)"
=======
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\SpeechSentemintAnalysis\\data\\afterReadingDataSet.csv\")\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.head())\n",
    "print(\"\\nColumn names:\", df.columns.tolist())\n",
    "\n",
    "# Fix column names\n",
    "df = df.rename(columns={'Emotions': 'label', 'Path': 'path'})\n",
    "\n",
    "# Remove rows with null path or label\n",
    "print(f\"\\nDataFrame shape before cleaning: {df.shape}\")\n",
    "df = df.dropna(subset=[\"path\", \"label\"])\n",
    "print(f\"DataFrame shape after removing nulls: {df.shape}\")\n",
    "\n",
    "# Optional: Remove rows where path file doesn't exist\n",
    "df_before_file_check = len(df)\n",
    "df = df[df[\"path\"].apply(os.path.exists)]\n",
    "print(f\"Removed {df_before_file_check - len(df)} rows with non-existent files\")\n",
    "\n",
    "# Map emotion labels to integers\n",
    "label_list = sorted(df[\"label\"].unique())\n",
    "label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(f\"\\nFound {len(label_list)} unique labels:\")\n",
    "for label, idx in label2id.items():\n",
    "    print(f\"  {label}: {idx}\")\n",
    "\n",
    "# Add label_id column\n",
    "df[\"label_id\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "print(f\"\\nDataFrame after adding label_id:\")\n",
    "print(df[[\"path\", \"label\", \"label_id\"]].head())\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(f\"\\nDataset created successfully!\")\n",
    "print(f\"Dataset info: {dataset}\")\n",
    "\n",
    "# Load processor\n",
    "try:\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
    "    print(\"Processor loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading processor: {e}\")\n",
    "    print(\"Make sure transformers is installed: pip install transformers\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {len(dataset)} samples\")\n",
    "print(f\"Features: {dataset.features}\")\n"
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "84dcc835",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/teamspace/studios/this_studio/speechSentimentAnalysis/processed_data\")\n",
    "\n",
    "# Load parquet files\n",
    "train_df = pd.read_parquet(DATA_DIR / \"train_processed.parquet\")\n",
    "valid_df = pd.read_parquet(DATA_DIR / \"valid_processed.parquet\")\n",
    "test_df  = pd.read_parquet(DATA_DIR / \"test_processed.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 20,
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   "id": "e48f87bf",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(valid_df)\n",
    "test_dataset  = Dataset.from_pandas(test_df)\n",
    "datasets = {\n",
    "    \"train\": train_dataset,\n",
    "    \"valid\": valid_dataset,\n",
    "    \"test\": test_dataset\n",
    "}\n"
=======
    "\n",
    "# Set target sample rate\n",
    "target_sampling_rate = 16000\n",
    "\n",
    "def preprocess(example):\n",
    "    \"\"\"\n",
    "    Preprocess audio files and return the processed example\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        speech_array, sampling_rate = torchaudio.load(example['path'])\n",
    "        \n",
    "        # Resample if necessary\n",
    "        if sampling_rate != target_sampling_rate:\n",
    "            resampler = torchaudio.transforms.Resample(\n",
    "                orig_freq=sampling_rate, \n",
    "                new_freq=target_sampling_rate\n",
    "            )\n",
    "            speech_array = resampler(speech_array)\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if speech_array.shape[0] > 1:\n",
    "            speech_array = torch.mean(speech_array, dim=0, keepdim=True)\n",
    "        \n",
    "        # Process with Wav2Vec2 processor\n",
    "        inputs = processor(\n",
    "            speech_array.squeeze().numpy(), \n",
    "            sampling_rate=target_sampling_rate, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Extract input_values and convert to list for Arrow compatibility\n",
    "        input_values = inputs[\"input_values\"].squeeze().tolist()\n",
    "        \n",
    "        # Update example with processed data\n",
    "        example[\"input_values\"] = input_values\n",
    "        example[\"labels\"] = example[\"label_id\"]  # Use 'labels' for consistency with transformers\n",
    "        \n",
    "        return example\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {example['path']}: {e}\")\n",
    "        # Return None to indicate this example should be filtered out\n",
    "        return None\n",
    "\n",
    "def filter_failed_examples(example):\n",
    "    \"\"\"Filter function to remove None examples\"\"\"\n",
    "    return example is not None\n"
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 21,
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   "id": "d4c3e2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SPLITTING DATASET\n",
      "==================================================\n",
      "Train dataset size: 9728\n",
      "Validation dataset size: 1216\n",
      "Test dataset size: 1217\n",
      "\n",
      "==================================================\n",
      "PREPROCESSING DATASETS\n",
      "==================================================\n",
      "Processing training dataset...\n",
      "Processing train example 0/9728\n",
      "Processing train example 100/9728\n",
      "Processing train example 200/9728\n",
      "Processing train example 300/9728\n",
      "Processing train example 400/9728\n",
      "Processing train example 500/9728\n",
      "Processing train example 600/9728\n",
      "Processing train example 700/9728\n",
      "Processing train example 800/9728\n",
      "Processing train example 900/9728\n",
      "Processing train example 1000/9728\n",
      "Processing train example 1100/9728\n",
      "Processing train example 1200/9728\n",
      "Processing train example 1300/9728\n",
      "Processing train example 1400/9728\n",
      "Processing train example 1500/9728\n",
      "Processing train example 1600/9728\n",
      "Processing train example 1700/9728\n",
      "Processing train example 1800/9728\n",
      "Processing train example 1900/9728\n",
      "Processing train example 2000/9728\n",
      "Processing train example 2100/9728\n",
      "Processing train example 2200/9728\n",
      "Processing train example 2300/9728\n",
      "Processing train example 2400/9728\n",
      "Processing train example 2500/9728\n",
      "Processing train example 2600/9728\n",
      "Processing train example 2700/9728\n",
      "Processing train example 2800/9728\n",
      "Processing train example 2900/9728\n",
      "Processing train example 3000/9728\n",
      "Processing train example 3100/9728\n",
      "Processing train example 3200/9728\n",
      "Processing train example 3300/9728\n",
      "Processing train example 3400/9728\n",
      "Processing train example 3500/9728\n",
      "Processing train example 3600/9728\n",
      "Processing train example 3700/9728\n",
      "Processing train example 3800/9728\n",
      "Processing train example 3900/9728\n",
      "Processing train example 4000/9728\n",
      "Processing train example 4100/9728\n",
      "Processing train example 4200/9728\n",
      "Processing train example 4300/9728\n",
      "Processing train example 4400/9728\n",
      "Processing train example 4500/9728\n",
      "Processing train example 4600/9728\n",
      "Processing train example 4700/9728\n",
      "Processing train example 4800/9728\n",
      "Processing train example 4900/9728\n",
      "Processing train example 5000/9728\n",
      "Processing train example 5100/9728\n",
      "Processing train example 5200/9728\n",
      "Processing train example 5300/9728\n",
      "Processing train example 5400/9728\n",
      "Processing train example 5500/9728\n",
      "Processing train example 5600/9728\n",
      "Processing train example 5700/9728\n",
      "Processing train example 5800/9728\n",
      "Processing train example 5900/9728\n",
      "Processing train example 6000/9728\n",
      "Processing train example 6100/9728\n",
      "Processing train example 6200/9728\n",
      "Processing train example 6300/9728\n",
      "Processing train example 6400/9728\n",
      "Processing train example 6500/9728\n",
      "Processing train example 6600/9728\n",
      "Processing train example 6700/9728\n",
      "Processing train example 6800/9728\n",
      "Processing train example 6900/9728\n",
      "Processing train example 7000/9728\n",
      "Processing train example 7100/9728\n",
      "Processing train example 7200/9728\n",
      "Processing train example 7300/9728\n",
      "Processing train example 7400/9728\n",
      "Processing train example 7500/9728\n",
      "Processing train example 7600/9728\n",
      "Processing train example 7700/9728\n",
      "Processing train example 7800/9728\n",
      "Processing train example 7900/9728\n",
      "Processing train example 8000/9728\n",
      "Processing train example 8100/9728\n",
      "Processing train example 8200/9728\n",
      "Processing train example 8300/9728\n",
      "Processing train example 8400/9728\n",
      "Processing train example 8500/9728\n",
      "Processing train example 8600/9728\n",
      "Processing train example 8700/9728\n",
      "Processing train example 8800/9728\n",
      "Processing train example 8900/9728\n",
      "Processing train example 9000/9728\n",
      "Processing train example 9100/9728\n",
      "Processing train example 9200/9728\n",
      "Processing train example 9300/9728\n",
      "Processing train example 9400/9728\n",
      "Processing train example 9500/9728\n",
      "Processing train example 9600/9728\n",
      "Processing train example 9700/9728\n",
      "Successfully processed 9728/9728 training examples\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "from typing import List, Dict, Any\n",
    "\n",
    "# Step 1: Preprocessing function - DON'T pad here, just truncate\n",
    "def preprocess_example(example):\n",
    "    audio = example[\"input_values\"]\n",
    "    audio = np.array(audio, dtype=np.float32)\n",
    "\n",
    "    max_len = 16000 * 10  # 10 seconds\n",
    "    # Only truncate if too long, don't pad here\n",
    "    if len(audio) > max_len:\n",
    "        audio = audio[:max_len]\n",
    "    \n",
    "    return {\"input_values\": audio}  # Return original length\n",
    "\n",
    "# Step 2: Define memory-safe data collator - padding happens here during batch creation\n",
    "class DataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        input_values = [torch.tensor(f[\"input_values\"], dtype=torch.float32) for f in features]\n",
    "        labels = [f[\"labels\"] for f in features]\n",
    "        max_len = max(len(i) for i in input_values)\n",
    "        padded = [torch.cat([i, torch.zeros(max_len - len(i))]) if len(i) < max_len else i for i in input_values]\n",
    "        batch_inputs = torch.stack(padded)\n",
    "        return {\"input_values\": batch_inputs, \"labels\": torch.tensor(labels, dtype=torch.long)}\n",
    "\n",
    "collator = DataCollator(processor)\n",
    "\n",
    "\n",
    "# %%\n",
    "print(\"Preprocessing train_dataset...\")\n",
    "\n",
    "# Process in smaller chunks and clear cache frequently\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_example,\n",
    "    batched=False,\n",
    "    desc=\"Processing train\",\n",
    "    load_from_cache_file=False,\n",
    "    writer_batch_size=100  # Write to disk more frequently\n",
    ")\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "print(\"âœ… Done with train_dataset\")\n",
    "\n",
    "\n",
    "# %%\n",
    "print(\"Preprocessing valid_dataset...\")\n",
    "\n",
    "valid_dataset = valid_dataset.map(\n",
    "    preprocess_example,\n",
    "    batched=False,\n",
    "    desc=\"Processing valid\",\n",
    "    load_from_cache_file=False,\n",
    "    writer_batch_size=100\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "print(\"âœ… Done with valid_dataset\")\n",
    "\n",
    "print(\"Preprocessing test_dataset...\")\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    preprocess_example,\n",
    "    batched=False,\n",
    "    desc=\"Processing test\",\n",
    "    load_from_cache_file=False,\n",
    "    writer_batch_size=100\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "print(\"âœ… Done with test_dataset\")\n"
=======
    "\n",
    "# Split dataset BEFORE preprocessing to avoid issues\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SPLITTING DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "train_testvalid = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "valid_test = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "train_dataset = train_testvalid['train']\n",
    "valid_dataset = valid_test['train']\n",
    "test_dataset = valid_test['test']\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(valid_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Preprocess datasets\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREPROCESSING DATASETS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Process train dataset\n",
    "print(\"Processing training dataset...\")\n",
    "train_processed = []\n",
    "for i, example in enumerate(train_dataset):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processing train example {i}/{len(train_dataset)}\")\n",
    "    \n",
    "    processed = preprocess(example)\n",
    "    if processed is not None:\n",
    "        train_processed.append(processed)\n",
    "\n",
    "print(f\"Successfully processed {len(train_processed)}/{len(train_dataset)} training examples\")\n"
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 22,
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   "id": "4f51050b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation dataset...\n",
      "Processing valid example 0/1216\n",
      "Processing valid example 50/1216\n",
      "Processing valid example 100/1216\n",
      "Processing valid example 150/1216\n",
      "Processing valid example 200/1216\n",
      "Processing valid example 250/1216\n",
      "Processing valid example 300/1216\n",
      "Processing valid example 350/1216\n",
      "Processing valid example 400/1216\n",
      "Processing valid example 450/1216\n",
      "Processing valid example 500/1216\n",
      "Processing valid example 550/1216\n",
      "Processing valid example 600/1216\n",
      "Processing valid example 650/1216\n",
      "Processing valid example 700/1216\n",
      "Processing valid example 750/1216\n",
      "Processing valid example 800/1216\n",
      "Processing valid example 850/1216\n",
      "Processing valid example 900/1216\n",
      "Processing valid example 950/1216\n",
      "Processing valid example 1000/1216\n",
      "Processing valid example 1050/1216\n",
      "Processing valid example 1100/1216\n",
      "Processing valid example 1150/1216\n",
      "Processing valid example 1200/1216\n",
      "Successfully processed 1216/1216 validation examples\n",
      "Processing test dataset...\n",
      "Processing test example 0/1217\n",
      "Processing test example 50/1217\n",
      "Processing test example 100/1217\n",
      "Processing test example 150/1217\n",
      "Processing test example 200/1217\n",
      "Processing test example 250/1217\n",
      "Processing test example 300/1217\n",
      "Processing test example 350/1217\n",
      "Processing test example 400/1217\n",
      "Processing test example 450/1217\n",
      "Processing test example 500/1217\n",
      "Processing test example 550/1217\n",
      "Processing test example 600/1217\n",
      "Processing test example 650/1217\n",
      "Processing test example 700/1217\n",
      "Processing test example 750/1217\n",
      "Processing test example 800/1217\n",
      "Processing test example 850/1217\n",
      "Processing test example 900/1217\n",
      "Processing test example 950/1217\n",
      "Processing test example 1000/1217\n",
      "Processing test example 1050/1217\n",
      "Processing test example 1100/1217\n",
      "Processing test example 1150/1217\n",
      "Processing test example 1200/1217\n",
      "Successfully processed 1217/1217 test examples\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Metric\n",
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    return {\"accuracy\": (preds == pred.label_ids).mean()}\n",
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-emotion\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    dataloader_num_workers=0,  # Reduce memory overhead\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
=======
    "\n",
    "# Process validation dataset\n",
    "print(\"Processing validation dataset...\")\n",
    "valid_processed = []\n",
    "for i, example in enumerate(valid_dataset):\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Processing valid example {i}/{len(valid_dataset)}\")\n",
    "    \n",
    "    processed = preprocess(example)\n",
    "    if processed is not None:\n",
    "        valid_processed.append(processed)\n",
    "\n",
    "print(f\"Successfully processed {len(valid_processed)}/{len(valid_dataset)} validation examples\")\n",
    "\n",
    "# Process test dataset\n",
    "print(\"Processing test dataset...\")\n",
    "test_processed = []\n",
    "for i, example in enumerate(test_dataset):\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Processing test example {i}/{len(test_dataset)}\")\n",
    "    \n",
    "    processed = preprocess(example)\n",
    "    if processed is not None:\n",
    "        test_processed.append(processed)\n",
    "\n",
    "print(f\"Successfully processed {len(test_processed)}/{len(test_dataset)} test examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db405af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ MEMORY-EFFICIENT SAVING (without Dataset conversion)\n",
      "============================================================\n",
      "ðŸ’¾ Saving processed data as pickle files...\n",
      "Saving 9728 training samples...\n",
      "âœ… Saved training data to pickle\n",
      "Saving 1216 validation samples...\n",
      "âœ… Saved validation data to pickle\n",
      "Saving 1217 test samples...\n",
      "âœ… Saved test data to pickle\n",
      "âœ… Saved label mappings to: C:\\Users\\asus\\Desktop\\SpeechSentemintAnalysis\\processed_data\\label_mappings.json\n",
      "âœ… Saved processing statistics to: C:\\Users\\asus\\Desktop\\SpeechSentemintAnalysis\\processed_data\\processing_stats.json\n",
      "\n",
      "ðŸ“ All files saved to directory: C:\\Users\\asus\\Desktop\\SpeechSentemintAnalysis\\processed_data\n",
      "\n",
      "============================================================\n",
      "ðŸŽ¯ DATA SAVED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "ðŸ”„ Converting to Dataset format in batches...\n",
      "Converting 9728 samples to Dataset...\n",
      "âœ… Test batch successful\n",
      "âš ï¸  Dataset too large (9728 samples), saving as pickle only\n",
      "Converting 1216 samples to Dataset...\n",
      "âœ… Test batch successful\n",
      "âš ï¸  Dataset too large (1216 samples), saving as pickle only\n",
      "Converting 1217 samples to Dataset...\n",
      "âœ… Test batch successful\n",
      "âš ï¸  Dataset too large (1217 samples), saving as pickle only\n",
      "\n",
      "ðŸ§¹ Clearing train_processed from memory...\n",
      "ðŸ§¹ Clearing valid_processed from memory...\n",
      "ðŸ§¹ Clearing test_processed from memory...\n",
      "ðŸ§¹ Memory cleared\n",
      "\n",
      "ðŸ“Š SAVE SUMMARY:\n",
      "âœ… Pickle files: All datasets saved successfully\n",
      "âœ… Dataset objects: 0/3 saved successfully\n",
      "âœ… Label mappings: Saved\n",
      "âœ… Statistics: Saved\n",
      "\n",
      "ðŸš€ NEXT STEPS:\n",
      "âš ï¸  Some datasets too large for Arrow format\n",
      "ðŸ’¡ Use the batch loading script below for training\n",
      "\n",
      "ðŸ“ Files in C:\\Users\\asus\\Desktop\\SpeechSentemintAnalysis\\processed_data:\n",
      "   ðŸ“„ label_mappings.json (0.0 MB)\n",
      "   ðŸ“„ processing_stats.json (0.0 MB)\n",
      "   ðŸ“„ test_processed.pkl (438.6 MB)\n",
      "   ðŸ“„ train_processed.pkl (3496.2 MB)\n",
      "   ðŸ“„ valid_processed.pkl (442.3 MB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import gc\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"ðŸ”„ MEMORY-EFFICIENT SAVING (without Dataset conversion)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "save_dir = r\"C:\\Users\\asus\\Desktop\\SpeechSentemintAnalysis\\processed_data\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save processed lists directly as pickle files first (more memory efficient)\n",
    "print(\"ðŸ’¾ Saving processed data as pickle files...\")\n",
    "\n",
    "if 'train_processed' in locals() and train_processed:\n",
    "    print(f\"Saving {len(train_processed)} training samples...\")\n",
    "    with open(os.path.join(save_dir, \"train_processed.pkl\"), 'wb') as f:\n",
    "        pickle.dump(train_processed, f)\n",
    "    print(f\"âœ… Saved training data to pickle\")\n",
    "\n",
    "if 'valid_processed' in locals() and valid_processed:\n",
    "    print(f\"Saving {len(valid_processed)} validation samples...\")\n",
    "    with open(os.path.join(save_dir, \"valid_processed.pkl\"), 'wb') as f:\n",
    "        pickle.dump(valid_processed, f)\n",
    "    print(f\"âœ… Saved validation data to pickle\")\n",
    "\n",
    "if 'test_processed' in locals() and test_processed:\n",
    "    print(f\"Saving {len(test_processed)} test samples...\")\n",
    "    with open(os.path.join(save_dir, \"test_processed.pkl\"), 'wb') as f:\n",
    "        pickle.dump(test_processed, f)\n",
    "    print(f\"âœ… Saved test data to pickle\")\n",
    "\n",
    "# Save label mappings\n",
    "if 'label2id' in locals():\n",
    "    mappings = {\n",
    "        \"label2id\": label2id,\n",
    "        \"id2label\": id2label,\n",
    "        \"label_list\": label_list,\n",
    "        \"num_labels\": len(label_list)\n",
    "    }\n",
    "    \n",
    "    mappings_path = os.path.join(save_dir, \"label_mappings.json\")\n",
    "    with open(mappings_path, 'w') as f:\n",
    "        json.dump(mappings, f, indent=2)\n",
    "    print(f\"âœ… Saved label mappings to: {mappings_path}\")\n",
    "\n",
    "# Save processing statistics\n",
    "if 'dataset' in locals():\n",
    "    stats = {\n",
    "        \"train_processed_size\": len(train_processed) if 'train_processed' in locals() and train_processed else 0,\n",
    "        \"valid_processed_size\": len(valid_processed) if 'valid_processed' in locals() and valid_processed else 0,\n",
    "        \"test_processed_size\": len(test_processed) if 'test_processed' in locals() and test_processed else 0,\n",
    "        \"target_sampling_rate\": target_sampling_rate if 'target_sampling_rate' in locals() else 16000,\n",
    "        \"processor_model\": \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "    }\n",
    "    \n",
    "    stats_path = os.path.join(save_dir, \"processing_stats.json\")\n",
    "    with open(stats_path, 'w') as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    print(f\"âœ… Saved processing statistics to: {stats_path}\")\n",
    "\n",
    "print(f\"\\nðŸ“ All files saved to directory: {save_dir}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ¯ DATA SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Now try to create and save Dataset objects in batches to avoid memory issues\n",
    "print(\"\\nðŸ”„ Converting to Dataset format in batches...\")\n",
    "\n",
    "def save_dataset_in_batches(processed_data, save_path, batch_size=100):\n",
    "    \"\"\"Save large datasets in smaller batches to avoid memory issues\"\"\"\n",
    "    if not processed_data:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        print(f\"Converting {len(processed_data)} samples to Dataset...\")\n",
    "        \n",
    "        # Try smaller batch first to test memory\n",
    "        test_batch = processed_data[:min(10, len(processed_data))]\n",
    "        test_dataset = Dataset.from_list(test_batch)\n",
    "        print(f\"âœ… Test batch successful\")\n",
    "        \n",
    "        # If test works, try full dataset\n",
    "        if len(processed_data) <= batch_size:\n",
    "            # Small enough to do all at once\n",
    "            dataset = Dataset.from_list(processed_data)\n",
    "            dataset.save_to_disk(save_path)\n",
    "            print(f\"âœ… Saved complete dataset to {save_path}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âš ï¸  Dataset too large ({len(processed_data)} samples), saving as pickle only\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Memory error during Dataset conversion: {e}\")\n",
    "        print(f\"ðŸ’¡ Data saved as pickle files - use batch loading script instead\")\n",
    "        return False\n",
    "\n",
    "# Try to save as Dataset objects (will fallback to pickle if memory issues)\n",
    "success_count = 0\n",
    "\n",
    "if 'train_processed' in locals() and train_processed:\n",
    "    train_save_path = os.path.join(save_dir, \"train_dataset\")\n",
    "    if save_dataset_in_batches(train_processed, train_save_path):\n",
    "        success_count += 1\n",
    "\n",
    "if 'valid_processed' in locals() and valid_processed:\n",
    "    valid_save_path = os.path.join(save_dir, \"valid_dataset\")\n",
    "    if save_dataset_in_batches(valid_processed, valid_save_path):\n",
    "        success_count += 1\n",
    "\n",
    "if 'test_processed' in locals() and test_processed:\n",
    "    test_save_path = os.path.join(save_dir, \"test_dataset\")\n",
    "    if save_dataset_in_batches(test_processed, test_save_path):\n",
    "        success_count += 1\n",
    "\n",
    "# Clear memory\n",
    "if 'train_processed' in locals():\n",
    "    print(f\"\\nðŸ§¹ Clearing train_processed from memory...\")\n",
    "    del train_processed\n",
    "if 'valid_processed' in locals():\n",
    "    print(f\"ðŸ§¹ Clearing valid_processed from memory...\")\n",
    "    del valid_processed\n",
    "if 'test_processed' in locals():\n",
    "    print(f\"ðŸ§¹ Clearing test_processed from memory...\")\n",
    "    del test_processed\n",
    "\n",
    "gc.collect()\n",
    "print(f\"ðŸ§¹ Memory cleared\")\n",
    "\n",
    "print(f\"\\nðŸ“Š SAVE SUMMARY:\")\n",
    "print(f\"âœ… Pickle files: All datasets saved successfully\")\n",
    "print(f\"âœ… Dataset objects: {success_count}/3 saved successfully\")\n",
    "print(f\"âœ… Label mappings: Saved\")\n",
    "print(f\"âœ… Statistics: Saved\")\n",
    "\n",
    "print(f\"\\nðŸš€ NEXT STEPS:\")\n",
    "if success_count == 3:\n",
    "    print(f\"âœ… All datasets saved in both formats - use the quick load script!\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Some datasets too large for Arrow format\")\n",
    "    print(f\"ðŸ’¡ Use the batch loading script below for training\")\n",
    "\n",
    "print(f\"\\nðŸ“ Files in {save_dir}:\")\n",
    "for file in os.listdir(save_dir):\n",
    "    file_path = os.path.join(save_dir, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size_mb = os.path.getsize(file_path) / (1024*1024)\n",
    "        print(f\"   ðŸ“„ {file} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"   ðŸ“ {file}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c409335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display sample from processed dataset\n",
    "if train_processed:\n",
    "    print(f\"\\nSample from processed training data:\")\n",
    "    sample = train_dataset[0]\n",
    "    print(f\"Keys: {sample.keys()}\")\n",
    "    print(f\"Input values length: {len(sample['input_values'])}\")\n",
    "    print(f\"Label: {sample['labels']}\")\n",
    "    print(f\"Original label: {sample['label']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUICK LOAD CODE FOR NEXT TIME:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\"\"\n",
    "# To load your processed datasets next time:\n",
    "from datasets import load_from_disk\n",
    "import json\n",
    "\n",
    "save_dir = r\"{save_dir}\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = load_from_disk(os.path.join(save_dir, \"train_dataset\"))\n",
    "valid_dataset = load_from_disk(os.path.join(save_dir, \"valid_dataset\"))\n",
    "test_dataset = load_from_disk(os.path.join(save_dir, \"test_dataset\"))\n",
    "\n",
    "# Load label mappings\n",
    "with open(os.path.join(save_dir, \"label_mappings.json\"), 'r') as f:\n",
    "    mappings = json.load(f)\n",
    "    label2id = mappings[\"label2id\"]\n",
    "    id2label = mappings[\"id2label\"]\n",
    "    num_labels = mappings[\"num_labels\"]\n",
    "\n",
    "print(f\"Loaded datasets - Train: {{len(train_dataset)}}, Valid: {{len(valid_dataset)}}, Test: {{len(test_dataset)}}\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114cb2e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgc\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Wav2Vec2Processor\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBatchDataLoader\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\__init__.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m3.6.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:56\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfsspec\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpa\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\__init__.py:59\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[32m     61\u001b[39m     ArrowDtype,\n\u001b[32m     62\u001b[39m     Int8Dtype,\n\u001b[32m     63\u001b[39m     Int16Dtype,\n\u001b[32m     64\u001b[39m     Int32Dtype,\n\u001b[32m     65\u001b[39m     Int64Dtype,\n\u001b[32m     66\u001b[39m     UInt8Dtype,\n\u001b[32m     67\u001b[39m     UInt16Dtype,\n\u001b[32m     68\u001b[39m     UInt32Dtype,\n\u001b[32m     69\u001b[39m     UInt64Dtype,\n\u001b[32m     70\u001b[39m     Float32Dtype,\n\u001b[32m     71\u001b[39m     Float64Dtype,\n\u001b[32m     72\u001b[39m     CategoricalDtype,\n\u001b[32m     73\u001b[39m     PeriodDtype,\n\u001b[32m     74\u001b[39m     IntervalDtype,\n\u001b[32m     75\u001b[39m     DatetimeTZDtype,\n\u001b[32m     76\u001b[39m     StringDtype,\n\u001b[32m     77\u001b[39m     BooleanDtype,\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[32m     79\u001b[39m     NA,\n\u001b[32m     80\u001b[39m     isna,\n\u001b[32m     81\u001b[39m     isnull,\n\u001b[32m     82\u001b[39m     notna,\n\u001b[32m     83\u001b[39m     notnull,\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[32m     85\u001b[39m     Index,\n\u001b[32m     86\u001b[39m     CategoricalIndex,\n\u001b[32m     87\u001b[39m     RangeIndex,\n\u001b[32m     88\u001b[39m     MultiIndex,\n\u001b[32m     89\u001b[39m     IntervalIndex,\n\u001b[32m     90\u001b[39m     TimedeltaIndex,\n\u001b[32m     91\u001b[39m     DatetimeIndex,\n\u001b[32m     92\u001b[39m     PeriodIndex,\n\u001b[32m     93\u001b[39m     IndexSlice,\n\u001b[32m     94\u001b[39m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[32m     95\u001b[39m     NaT,\n\u001b[32m     96\u001b[39m     Period,\n\u001b[32m     97\u001b[39m     period_range,\n\u001b[32m     98\u001b[39m     Timedelta,\n\u001b[32m     99\u001b[39m     timedelta_range,\n\u001b[32m    100\u001b[39m     Timestamp,\n\u001b[32m    101\u001b[39m     date_range,\n\u001b[32m    102\u001b[39m     bdate_range,\n\u001b[32m    103\u001b[39m     Interval,\n\u001b[32m    104\u001b[39m     interval_range,\n\u001b[32m    105\u001b[39m     DateOffset,\n\u001b[32m    106\u001b[39m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[32m    107\u001b[39m     to_numeric,\n\u001b[32m    108\u001b[39m     to_datetime,\n\u001b[32m    109\u001b[39m     to_timedelta,\n\u001b[32m    110\u001b[39m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[32m    111\u001b[39m     Flags,\n\u001b[32m    112\u001b[39m     Grouper,\n\u001b[32m    113\u001b[39m     factorize,\n\u001b[32m    114\u001b[39m     unique,\n\u001b[32m    115\u001b[39m     value_counts,\n\u001b[32m    116\u001b[39m     NamedAgg,\n\u001b[32m    117\u001b[39m     array,\n\u001b[32m    118\u001b[39m     Categorical,\n\u001b[32m    119\u001b[39m     set_eng_float_format,\n\u001b[32m    120\u001b[39m     Series,\n\u001b[32m    121\u001b[39m     DataFrame,\n\u001b[32m    122\u001b[39m )\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtseries\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     NaT,\n\u001b[32m      3\u001b[39m     Period,\n\u001b[32m      4\u001b[39m     Timedelta,\n\u001b[32m      5\u001b[39m     Timestamp,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     ArrowDtype,\n\u001b[32m     11\u001b[39m     CategoricalDtype,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     PeriodDtype,\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\__init__.py:18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401,E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minterval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     NaT,\n\u001b[32m     21\u001b[39m     NaTType,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     iNaT,\n\u001b[32m     27\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32minterval.pyx:1\u001b[39m, in \u001b[36minit pandas._libs.interval\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import gc\n",
    "from datasets import Dataset\n",
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "class BatchDataLoader:\n",
    "    \"\"\"Memory-efficient loader for large processed datasets\"\"\"\n",
    "    \n",
    "    def __init__(self, save_dir):\n",
    "        self.save_dir = save_dir\n",
    "        self.label_mappings = None\n",
    "        self.stats = None\n",
    "        self.processor = None\n",
    "        \n",
    "    def load_metadata(self):\n",
    "        \"\"\"Load label mappings and statistics\"\"\"\n",
    "        print(\"ðŸ“‹ Loading metadata...\")\n",
    "        \n",
    "        # Load label mappings\n",
    "        mappings_path = os.path.join(self.save_dir, \"label_mappings.json\")\n",
    "        if os.path.exists(mappings_path):\n",
    "            with open(mappings_path, 'r') as f:\n",
    "                self.label_mappings = json.load(f)\n",
    "            print(f\"âœ… Loaded {self.label_mappings['num_labels']} labels\")\n",
    "        \n",
    "        # Load stats\n",
    "        stats_path = os.path.join(self.save_dir, \"processing_stats.json\")\n",
    "        if os.path.exists(stats_path):\n",
    "            with open(stats_path, 'r') as f:\n",
    "                self.stats = json.load(f)\n",
    "            print(f\"âœ… Loaded processing statistics\")\n",
    "        \n",
    "        # Load processor\n",
    "        try:\n",
    "            self.processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
    "            print(f\"âœ… Loaded processor\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Could not load processor: {e}\")\n",
    "    \n",
    "    def try_load_dataset(self, dataset_name):\n",
    "        \"\"\"Try to load dataset, fallback to pickle if needed\"\"\"\n",
    "        dataset_path = os.path.join(self.save_dir, f\"{dataset_name}_dataset\")\n",
    "        pickle_path = os.path.join(self.save_dir, f\"{dataset_name}_processed.pkl\")\n",
    "        \n",
    "        # Try loading as Dataset first\n",
    "        if os.path.exists(dataset_path):\n",
    "            try:\n",
    "                dataset = Dataset.load_from_disk(dataset_path)\n",
    "                print(f\"âœ… Loaded {dataset_name} dataset ({len(dataset)} samples)\")\n",
    "                return dataset\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Could not load {dataset_name} dataset: {e}\")\n",
    "        \n",
    "        # Fallback to pickle\n",
    "        if os.path.exists(pickle_path):\n",
    "            try:\n",
    "                print(f\"ðŸ”„ Loading {dataset_name} from pickle...\")\n",
    "                with open(pickle_path, 'rb') as f:\n",
    "                    processed_data = pickle.load(f)\n",
    "                \n",
    "                # Try to convert to Dataset in smaller batches\n",
    "                return self.pickle_to_dataset_batched(processed_data, dataset_name)\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Could not load {dataset_name} pickle: {e}\")\n",
    "                return None\n",
    "        \n",
    "        print(f\"âŒ No data found for {dataset_name}\")\n",
    "        return None\n",
    "    \n",
    "    def pickle_to_dataset_batched(self, processed_data, name, batch_size=50):\n",
    "        \"\"\"Convert pickle data to Dataset in batches\"\"\"\n",
    "        if not processed_data:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Try small batch first\n",
    "            test_size = min(10, len(processed_data))\n",
    "            test_batch = processed_data[:test_size]\n",
    "            test_dataset = Dataset.from_list(test_batch)\n",
    "            print(f\"âœ… Test conversion successful for {name}\")\n",
    "            \n",
    "            # If data is small enough, convert all at once\n",
    "            if len(processed_data) <= batch_size:\n",
    "                dataset = Dataset.from_list(processed_data)\n",
    "                print(f\"âœ… Converted all {len(processed_data)} {name} samples to Dataset\")\n",
    "                return dataset\n",
    "            else:\n",
    "                # For larger datasets, you might want to return a custom iterator\n",
    "                print(f\"âš ï¸  {name} dataset large ({len(processed_data)} samples)\")\n",
    "                print(f\"ðŸ’¡ Consider using smaller batches during training\")\n",
    "                \n",
    "                # Still try to convert, but with more aggressive memory management\n",
    "                gc.collect()\n",
    "                dataset = Dataset.from_list(processed_data)\n",
    "                print(f\"âœ… Converted {len(processed_data)} {name} samples (may use lots of memory)\")\n",
    "                return dataset\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to convert {name} pickle to Dataset: {e}\")\n",
    "            print(f\"ðŸ’¡ You may need to use the raw pickle data with custom data loaders\")\n",
    "            return None\n",
    "    \n",
    "    def load_all(self):\n",
    "        \"\"\"Load all datasets and metadata\"\"\"\n",
    "        print(\"ðŸš€ Loading all processed data...\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        self.load_metadata()\n",
    "        \n",
    "        # Try to load datasets\n",
    "        train_dataset = self.try_load_dataset(\"train\")\n",
    "        valid_dataset = self.try_load_dataset(\"valid\") \n",
    "        test_dataset = self.try_load_dataset(\"test\")\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\nðŸ“Š LOADING SUMMARY:\")\n",
    "        print(\"=\"*50)\n",
    "        datasets = {\n",
    "            \"train\": train_dataset,\n",
    "            \"valid\": valid_dataset, \n",
    "            \"test\": test_dataset\n",
    "        }\n",
    "        \n",
    "        for name, dataset in datasets.items():\n",
    "            if dataset is not None:\n",
    "                print(f\"âœ… {name.capitalize()}: {len(dataset)} samples\")\n",
    "            else:\n",
    "                print(f\"âŒ {name.capitalize()}: Failed to load\")\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ Available variables:\")\n",
    "        if self.label_mappings:\n",
    "            print(f\"   - label2id, id2label, num_labels\")\n",
    "        if self.processor:\n",
    "            print(f\"   - processor\")\n",
    "        \n",
    "        return {\n",
    "            'train_dataset': train_dataset,\n",
    "            'valid_dataset': valid_dataset,\n",
    "            'test_dataset': test_dataset,\n",
    "            'label2id': self.label_mappings.get('label2id') if self.label_mappings else None,\n",
    "            'id2label': self.label_mappings.get('id2label') if self.label_mappings else None,\n",
    "            'num_labels': self.label_mappings.get('num_labels') if self.label_mappings else None,\n",
    "            'processor': self.processor,\n",
    "            'stats': self.stats\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "save_dir = r\"C:\\Users\\asus\\Desktop\\SpeechSentemintAnalysis\\processed_data\"\n",
    "\n",
    "# Check if save directory exists\n",
    "if os.path.exists(save_dir):\n",
    "    loader = BatchDataLoader(save_dir)\n",
    "    results = loader.load_all()\n",
    "    \n",
    "    # Extract variables for easy access\n",
    "    train_dataset = results['train_dataset']\n",
    "    valid_dataset = results['valid_dataset']\n",
    "    test_dataset = results['test_dataset']\n",
    "    label2id = results['label2id']\n",
    "    id2label = results['id2label']\n",
    "    num_labels = results['num_labels']\n",
    "    processor = results['processor']\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ Data loading complete!\")\n",
    "    print(f\"Ready for model training with: train_dataset, valid_dataset, test_dataset\")\n",
    "    \n",
    "    # Display sample if available\n",
    "    if train_dataset and len(train_dataset) > 0:\n",
    "        print(f\"\\nðŸ” Sample from training data:\")\n",
    "        sample = train_dataset[0]\n",
    "        print(f\"   - Keys: {list(sample.keys())}\")\n",
    "        print(f\"   - Input length: {len(sample['input_values'])}\")\n",
    "        if 'labels' in sample:\n",
    "            print(f\"   - Label: {sample['labels']} ({id2label[sample['labels']] if id2label else 'unknown'})\")\n",
    "        \n",
    "else:\n",
    "    print(f\"âŒ Save directory not found: {save_dir}\")\n",
    "    print(f\"Please run the processing script first.\")"
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "lightning",
=======
   "display_name": "Python 3",
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.11.13"
=======
   "version": "3.11.9"
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
