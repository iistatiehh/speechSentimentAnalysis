{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "228e2ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from datasets import Dataset\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
    "import gc\n",
    "import numpy as np\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e46f5eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç System Resources:\n",
      "  CPU cores: 8\n",
      "  RAM: 15.7 GB total, 2.3 GB available\n",
      "  Recommended batch size: 4\n",
      "  GPU: Not available\n",
      "üéØ Using device: cpu\n",
      "üìä Optimized settings:\n",
      "  Batch size: 4\n",
      "  Max audio length: 4.0 seconds\n",
      "  Save frequency: Every 10 batches\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPTIMIZED CONFIGURATION FOR MEMORY EFFICIENCY\n",
    "# ============================================================================\n",
    "\n",
    "def check_resources():\n",
    "    \"\"\"Check available resources and recommend settings\"\"\"\n",
    "    print(\"üîç System Resources:\")\n",
    "    print(f\"  CPU cores: {psutil.cpu_count()}\")\n",
    "    ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "    available_ram_gb = psutil.virtual_memory().available / (1024**3)\n",
    "    print(f\"  RAM: {ram_gb:.1f} GB total, {available_ram_gb:.1f} GB available\")\n",
    "    \n",
    "    # Memory-based batch size recommendation\n",
    "    if available_ram_gb < 4:\n",
    "        recommended_batch = 4\n",
    "    elif available_ram_gb < 8:\n",
    "        recommended_batch = 8\n",
    "    elif available_ram_gb < 16:\n",
    "        recommended_batch = 16\n",
    "    else:\n",
    "        recommended_batch = 32\n",
    "    \n",
    "    print(f\"  Recommended batch size: {recommended_batch}\")\n",
    "    \n",
    "    # GPU check\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if device.type == 'cuda':\n",
    "        print(f\"  GPU: Available - {torch.cuda.get_device_name()}\")\n",
    "        print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "    else:\n",
    "        print(\"  GPU: Not available\")\n",
    "    \n",
    "    return device, recommended_batch\n",
    "\n",
    "device, RECOMMENDED_BATCH_SIZE = check_resources()\n",
    "\n",
    "# OPTIMIZED SETTINGS\n",
    "BATCH_SIZE = min(RECOMMENDED_BATCH_SIZE, 8)  # Conservative for stability\n",
    "MAX_AUDIO_LENGTH = 16000 * 4  # Reduced to 4 seconds (was 5)\n",
    "TARGET_SAMPLING_RATE = 16000\n",
    "SAVE_EVERY_N_BATCHES = 10  # Save progress more frequently\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path(\"/teamspace/studios/this_studio/speechSentimentAnalysis/processed_data\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üéØ Using device: {device}\")\n",
    "print(f\"üìä Optimized settings:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Max audio length: {MAX_AUDIO_LENGTH/16000:.1f} seconds\")\n",
    "print(f\"  Save frequency: Every {SAVE_EVERY_N_BATCHES} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c3660e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STREAMING PROCESSOR (MEMORY EFFICIENT)\n",
    "# ============================================================================\n",
    "\n",
    "class StreamingAudioProcessor:\n",
    "    \"\"\"\n",
    "    Memory-efficient streaming processor that saves intermediate results\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dir, batch_size=BATCH_SIZE):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.batch_size = batch_size\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
    "        self.processed_samples = []\n",
    "        self.batch_count = 0\n",
    "        \n",
    "    def process_single_file(self, file_path, label_id, label, dataset):\n",
    "        \"\"\"Process a single audio file\"\"\"\n",
    "        try:\n",
    "            # Load audio\n",
    "            speech_array, sr = load_audio_memory_efficient(file_path)\n",
    "            \n",
    "            # Process with Wav2Vec2 - WITHOUT truncation parameters\n",
    "            inputs = self.processor(\n",
    "                speech_array, \n",
    "                sampling_rate=sr, \n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            )\n",
    "            \n",
    "            # Extract and convert to list (save memory)\n",
    "            input_values = inputs['input_values'].squeeze().tolist()\n",
    "            \n",
    "            # Clean up tensors immediately\n",
    "            del inputs\n",
    "            del speech_array\n",
    "            \n",
    "            return {\n",
    "                'input_values': input_values,\n",
    "                'labels': int(label_id),\n",
    "                'label': label,\n",
    "                'dataset': dataset,\n",
    "                'path': file_path\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_batch(self, batch_samples, batch_idx):\n",
    "        \"\"\"Save a batch of processed samples\"\"\"\n",
    "        if not batch_samples:\n",
    "            return\n",
    "        \n",
    "        batch_df = pd.DataFrame(batch_samples)\n",
    "        batch_file = self.output_dir / f\"batch_{batch_idx:04d}.parquet\"\n",
    "        batch_df.to_parquet(batch_file)\n",
    "        \n",
    "        # Clear the batch from memory\n",
    "        del batch_df\n",
    "        del batch_samples\n",
    "        gc.collect()\n",
    "        \n",
    "        return batch_file\n",
    "    \n",
    "    def process_dataset_streaming(self, dataset_df, dataset_name):\n",
    "        \"\"\"Process dataset in streaming fashion\"\"\"\n",
    "        print(f\"\\nüîÑ Processing {dataset_name} in streaming mode...\")\n",
    "        \n",
    "        total_samples = len(dataset_df)\n",
    "        processed_count = 0\n",
    "        batch_files = []\n",
    "        current_batch = []\n",
    "        \n",
    "        for idx, row in dataset_df.iterrows():\n",
    "            # Process single file\n",
    "            sample = self.process_single_file(\n",
    "                row['path'], \n",
    "                row['label_id'], \n",
    "                row['label'], \n",
    "                row['dataset']\n",
    "            )\n",
    "            \n",
    "            if sample:\n",
    "                current_batch.append(sample)\n",
    "                processed_count += 1\n",
    "            \n",
    "            # Save batch when full or at end\n",
    "            if len(current_batch) >= self.batch_size or idx == total_samples - 1:\n",
    "                if current_batch:\n",
    "                    batch_file = self.save_batch(current_batch, self.batch_count)\n",
    "                    batch_files.append(batch_file)\n",
    "                    current_batch = []\n",
    "                    self.batch_count += 1\n",
    "                    \n",
    "                    # Progress update\n",
    "                    progress = (processed_count / total_samples) * 100\n",
    "                    print(f\"  Processed {processed_count}/{total_samples} ({progress:.1f}%)\")\n",
    "                    \n",
    "                    # Aggressive memory cleanup\n",
    "                    if self.batch_count % SAVE_EVERY_N_BATCHES == 0:\n",
    "                        gc.collect()\n",
    "                        if device.type == 'cuda':\n",
    "                            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"‚úÖ {dataset_name}: {processed_count} samples processed in {len(batch_files)} batches\")\n",
    "        return batch_files, processed_count\n",
    "    \n",
    "    def combine_batches(self, batch_files, output_name):\n",
    "        \"\"\"Combine batch files into final dataset\"\"\"\n",
    "        print(f\"\\nüîó Combining {len(batch_files)} batches into {output_name}...\")\n",
    "        \n",
    "        all_data = []\n",
    "        for batch_file in batch_files:\n",
    "            try:\n",
    "                batch_df = pd.read_parquet(batch_file)\n",
    "                all_data.append(batch_df)\n",
    "                \n",
    "                # Delete batch file to save space\n",
    "                batch_file.unlink()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error reading batch {batch_file}: {e}\")\n",
    "        \n",
    "        if all_data:\n",
    "            combined_df = pd.concat(all_data, ignore_index=True)\n",
    "            output_path = self.output_dir / f\"{output_name}.parquet\"\n",
    "            combined_df.to_parquet(output_path)\n",
    "            \n",
    "            print(f\"‚úÖ Combined dataset saved: {output_path}\")\n",
    "            print(f\"   Final size: {len(combined_df)} samples\")\n",
    "            \n",
    "            # Cleanup\n",
    "            del all_data\n",
    "            del combined_df\n",
    "            gc.collect()\n",
    "            \n",
    "            return output_path\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATASET LOADING (SAME AS ORIGINAL)\n",
    "# ============================================================================\n",
    "\n",
    "def load_datasets():\n",
    "    \"\"\"Load all datasets with file existence checking\"\"\"\n",
    "    BASE_DIR = \"/teamspace/studios/this_studio/speechSentimentAnalysis/data/input\"\n",
    "    all_entries = []\n",
    "    \n",
    "    print(f\"üìÇ Loading datasets from: {BASE_DIR}\")\n",
    "    \n",
    "    # RAVDESS\n",
    "    ravdess_path = os.path.join(BASE_DIR, \"Ravdess/audio_speech_actors_01-24\")\n",
    "    if os.path.exists(ravdess_path):\n",
    "        print(\"Loading RAVDESS...\")\n",
    "        for actor in os.listdir(ravdess_path):\n",
    "            actor_path = os.path.join(ravdess_path, actor)\n",
    "            if os.path.isdir(actor_path):\n",
    "                for file in os.listdir(actor_path):\n",
    "                    if file.endswith(\".wav\"):\n",
    "                        file_path = os.path.join(actor_path, file)\n",
    "                        if os.path.exists(file_path):  # Check file exists\n",
    "                            parts = file.split('.')[0].split('-')\n",
    "                            if len(parts) >= 3:\n",
    "                                emotion_id = int(parts[2])\n",
    "                                label_map = {\n",
    "                                    1: 'neutral', 2: 'calm', 3: 'happy', 4: 'sad',\n",
    "                                    5: 'angry', 6: 'fear', 7: 'disgust', 8: 'surprise'\n",
    "                                }\n",
    "                                label = label_map.get(emotion_id, 'unknown')\n",
    "                                all_entries.append({\n",
    "                                    \"path\": file_path,\n",
    "                                    \"label\": label,\n",
    "                                    \"dataset\": \"ravdess\"\n",
    "                                })\n",
    "    \n",
    "    # CREMA-D\n",
    "    crema_path = os.path.join(BASE_DIR, \"Crema\")\n",
    "    if os.path.exists(crema_path):\n",
    "        print(\"Loading CREMA-D...\")\n",
    "        for file in os.listdir(crema_path):\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(crema_path, file)\n",
    "                if os.path.exists(file_path):  # Check file exists\n",
    "                    parts = file.split('_')\n",
    "                    if len(parts) >= 3:\n",
    "                        emotion_code = parts[2]\n",
    "                        label_map = {\n",
    "                            'SAD': 'sad', 'ANG': 'angry', 'DIS': 'disgust',\n",
    "                            'FEA': 'fear', 'HAP': 'happy', 'NEU': 'neutral'\n",
    "                        }\n",
    "                        label = label_map.get(emotion_code, 'unknown')\n",
    "                        all_entries.append({\n",
    "                            \"path\": file_path,\n",
    "                            \"label\": label,\n",
    "                            \"dataset\": \"crema\"\n",
    "                        })\n",
    "    \n",
    "    # TESS\n",
    "    tess_path = os.path.join(BASE_DIR, \"Tess\")\n",
    "    if os.path.exists(tess_path):\n",
    "        print(\"Loading TESS...\")\n",
    "        for speaker_dir in os.listdir(tess_path):\n",
    "            speaker_path = os.path.join(tess_path, speaker_dir)\n",
    "            if os.path.isdir(speaker_path):\n",
    "                for file in os.listdir(speaker_path):\n",
    "                    if file.endswith(\".wav\"):\n",
    "                        file_path = os.path.join(speaker_path, file)\n",
    "                        if os.path.exists(file_path):  # Check file exists\n",
    "                            parts = file.split('.')[0].split('_')\n",
    "                            if len(parts) >= 3:\n",
    "                                emotion_code = parts[2]\n",
    "                                label = 'surprise' if emotion_code == 'ps' else emotion_code.lower()\n",
    "                                all_entries.append({\n",
    "                                    \"path\": file_path,\n",
    "                                    \"label\": label,\n",
    "                                    \"dataset\": \"tess\"\n",
    "                                })\n",
    "    \n",
    "    # SAVEE\n",
    "    savee_path = os.path.join(BASE_DIR, \"Savee\")\n",
    "    if os.path.exists(savee_path):\n",
    "        print(\"Loading SAVEE...\")\n",
    "        for file in os.listdir(savee_path):\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(savee_path, file)\n",
    "                if os.path.exists(file_path):  # Check file exists\n",
    "                    parts = file.split('_')\n",
    "                    if len(parts) >= 2:\n",
    "                        emo = parts[1]\n",
    "                        code = emo[:2] if emo.startswith('sa') else emo[0]\n",
    "                        label_map = {\n",
    "                            'a': 'angry', 'd': 'disgust', 'f': 'fear', 'h': 'happy',\n",
    "                            'n': 'neutral', 'sa': 'sad', 'su': 'surprise'\n",
    "                        }\n",
    "                        label = label_map.get(code, 'unknown')\n",
    "                        all_entries.append({\n",
    "                            \"path\": file_path,\n",
    "                            \"label\": label,\n",
    "                            \"dataset\": \"savee\"\n",
    "                        })\n",
    "    \n",
    "    # Create DataFrame and clean\n",
    "    df = pd.DataFrame(all_entries)\n",
    "    df = df[df[\"label\"] != \"unknown\"]\n",
    "    \n",
    "    # Create label mappings\n",
    "    label_list = sorted(df[\"label\"].unique())\n",
    "    label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "    id2label = {idx: label for label, idx in label2id.items()}\n",
    "    \n",
    "    df[\"label_id\"] = df[\"label\"].map(label2id)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(df)} total samples\")\n",
    "    print(f\"üìä Labels: {label_list}\")\n",
    "    \n",
    "    return df, label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "069a22ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Memory-Optimized Processing Pipeline\n",
      "============================================================\n",
      "üìÇ Loading datasets from: data\\input\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 99\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m memory_mb\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Load datasets\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m df, label2id, id2label = \u001b[43mload_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Save metadata\u001b[39;00m\n\u001b[32m     14\u001b[39m metadata_path = OUTPUT_DIR / \u001b[33m\"\u001b[39m\u001b[33mmetadata.csv\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mload_datasets\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Create DataFrame and clean\u001b[39;00m\n\u001b[32m    104\u001b[39m df = pd.DataFrame(all_entries)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m df = df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m != \u001b[33m\"\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# Create label mappings\u001b[39;00m\n\u001b[32m    108\u001b[39m label_list = \u001b[38;5;28msorted\u001b[39m(df[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m].unique())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3891\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   3892\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m3893\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3894\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   3895\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:418\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    416\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'label'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN PROCESSING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main processing pipeline\"\"\"\n",
    "    print(\"üöÄ Starting Memory-Optimized Processing Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load datasets\n",
    "    df, label2id, id2label = load_datasets()\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = OUTPUT_DIR / \"metadata.csv\"\n",
    "    df.to_csv(metadata_path, index=False)\n",
    "    \n",
    "    # Save label mappings\n",
    "    with open(OUTPUT_DIR / \"label_mappings.json\", \"w\") as f:\n",
    "        json.dump({\"label2id\": label2id, \"id2label\": id2label}, f, indent=2)\n",
    "    \n",
    "    # Split dataset\n",
    "    print(\"\\nüìä Splitting dataset...\")\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "    valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
    "    \n",
    "    print(f\"  Train: {len(train_df)} samples\")\n",
    "    print(f\"  Validation: {len(valid_df)} samples\")\n",
    "    print(f\"  Test: {len(test_df)} samples\")\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = StreamingAudioProcessor(OUTPUT_DIR, BATCH_SIZE)\n",
    "    \n",
    "    # Process each split\n",
    "    datasets = [\n",
    "        (train_df, \"train\"),\n",
    "        (valid_df, \"valid\"),\n",
    "        (test_df, \"test\")\n",
    "    ]\n",
    "    \n",
    "    for dataset_df, name in datasets:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing {name.upper()} dataset\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Process in streaming mode\n",
    "        batch_files, processed_count = processor.process_dataset_streaming(dataset_df, name)\n",
    "        \n",
    "        # Combine batches\n",
    "        if batch_files:\n",
    "            final_path = processor.combine_batches(batch_files, f\"{name}_processed\")\n",
    "            print(f\"‚úÖ {name} dataset: {processed_count} samples saved to {final_path}\")\n",
    "        \n",
    "        # Reset batch counter for next dataset\n",
    "        processor.batch_count = 0\n",
    "    \n",
    "    print(f\"\\nüéâ All processing complete!\")\n",
    "    print(f\"üìÅ Output directory: {OUTPUT_DIR}\")\n",
    "    print(f\"üìÑ Files created:\")\n",
    "    print(f\"  - metadata.csv\")\n",
    "    print(f\"  - label_mappings.json\")\n",
    "    print(f\"  - train_processed.parquet\")\n",
    "    print(f\"  - valid_processed.parquet\")\n",
    "    print(f\"  - test_processed.parquet\")\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def load_processed_datasets():\n",
    "    \"\"\"Load processed datasets for training\"\"\"\n",
    "    try:\n",
    "        train_df = pd.read_parquet(OUTPUT_DIR / \"train_processed.parquet\")\n",
    "        valid_df = pd.read_parquet(OUTPUT_DIR / \"valid_processed.parquet\")\n",
    "        test_df = pd.read_parquet(OUTPUT_DIR / \"test_processed.parquet\")\n",
    "        \n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        valid_dataset = Dataset.from_pandas(valid_df)\n",
    "        test_dataset = Dataset.from_pandas(test_df)\n",
    "        \n",
    "        print(f\"‚úÖ Loaded processed datasets:\")\n",
    "        print(f\"  Train: {len(train_dataset)} samples\")\n",
    "        print(f\"  Validation: {len(valid_dataset)} samples\")\n",
    "        print(f\"  Test: {len(test_dataset)} samples\")\n",
    "        \n",
    "        return train_dataset, valid_dataset, test_dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading processed datasets: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Check current memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "    print(f\"üíæ Current memory usage: {memory_mb:.1f} MB\")\n",
    "    return memory_mb\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
