{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 2,
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   "id": "783b1ff5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
<<<<<<< HEAD
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchaudio\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset, Dataset\n",
=======
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchaudio\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset, Dataset\n",
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\__init__.py:59\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[32m     61\u001b[39m     ArrowDtype,\n\u001b[32m     62\u001b[39m     Int8Dtype,\n\u001b[32m     63\u001b[39m     Int16Dtype,\n\u001b[32m     64\u001b[39m     Int32Dtype,\n\u001b[32m     65\u001b[39m     Int64Dtype,\n\u001b[32m     66\u001b[39m     UInt8Dtype,\n\u001b[32m     67\u001b[39m     UInt16Dtype,\n\u001b[32m     68\u001b[39m     UInt32Dtype,\n\u001b[32m     69\u001b[39m     UInt64Dtype,\n\u001b[32m     70\u001b[39m     Float32Dtype,\n\u001b[32m     71\u001b[39m     Float64Dtype,\n\u001b[32m     72\u001b[39m     CategoricalDtype,\n\u001b[32m     73\u001b[39m     PeriodDtype,\n\u001b[32m     74\u001b[39m     IntervalDtype,\n\u001b[32m     75\u001b[39m     DatetimeTZDtype,\n\u001b[32m     76\u001b[39m     StringDtype,\n\u001b[32m     77\u001b[39m     BooleanDtype,\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[32m     79\u001b[39m     NA,\n\u001b[32m     80\u001b[39m     isna,\n\u001b[32m     81\u001b[39m     isnull,\n\u001b[32m     82\u001b[39m     notna,\n\u001b[32m     83\u001b[39m     notnull,\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[32m     85\u001b[39m     Index,\n\u001b[32m     86\u001b[39m     CategoricalIndex,\n\u001b[32m     87\u001b[39m     RangeIndex,\n\u001b[32m     88\u001b[39m     MultiIndex,\n\u001b[32m     89\u001b[39m     IntervalIndex,\n\u001b[32m     90\u001b[39m     TimedeltaIndex,\n\u001b[32m     91\u001b[39m     DatetimeIndex,\n\u001b[32m     92\u001b[39m     PeriodIndex,\n\u001b[32m     93\u001b[39m     IndexSlice,\n\u001b[32m     94\u001b[39m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[32m     95\u001b[39m     NaT,\n\u001b[32m     96\u001b[39m     Period,\n\u001b[32m     97\u001b[39m     period_range,\n\u001b[32m     98\u001b[39m     Timedelta,\n\u001b[32m     99\u001b[39m     timedelta_range,\n\u001b[32m    100\u001b[39m     Timestamp,\n\u001b[32m    101\u001b[39m     date_range,\n\u001b[32m    102\u001b[39m     bdate_range,\n\u001b[32m    103\u001b[39m     Interval,\n\u001b[32m    104\u001b[39m     interval_range,\n\u001b[32m    105\u001b[39m     DateOffset,\n\u001b[32m    106\u001b[39m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[32m    107\u001b[39m     to_numeric,\n\u001b[32m    108\u001b[39m     to_datetime,\n\u001b[32m    109\u001b[39m     to_timedelta,\n\u001b[32m    110\u001b[39m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[32m    111\u001b[39m     Flags,\n\u001b[32m    112\u001b[39m     Grouper,\n\u001b[32m    113\u001b[39m     factorize,\n\u001b[32m    114\u001b[39m     unique,\n\u001b[32m    115\u001b[39m     value_counts,\n\u001b[32m    116\u001b[39m     NamedAgg,\n\u001b[32m    117\u001b[39m     array,\n\u001b[32m    118\u001b[39m     Categorical,\n\u001b[32m    119\u001b[39m     set_eng_float_format,\n\u001b[32m    120\u001b[39m     Series,\n\u001b[32m    121\u001b[39m     DataFrame,\n\u001b[32m    122\u001b[39m )\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtseries\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     NaT,\n\u001b[32m      3\u001b[39m     Period,\n\u001b[32m      4\u001b[39m     Timedelta,\n\u001b[32m      5\u001b[39m     Timestamp,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     ArrowDtype,\n\u001b[32m     11\u001b[39m     CategoricalDtype,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     PeriodDtype,\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\__init__.py:18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401,E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minterval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     NaT,\n\u001b[32m     21\u001b[39m     NaTType,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     iNaT,\n\u001b[32m     27\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32minterval.pyx:1\u001b[39m, in \u001b[36minit pandas._libs.interval\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
<<<<<<< HEAD
    "from datasets import Dataset\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification,DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "import gc\n",
    "import numpy as np\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
=======
    "from datasets import load_dataset, Dataset\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification, DataCollatorWithPadding\n"
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 19,
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   "id": "07fd1dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "  Emotions                                               Path\n",
      "0  neutral  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...\n",
      "1  neutral  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...\n",
      "2  neutral  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...\n",
      "3  neutral  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...\n",
      "4     calm  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...\n",
      "\n",
      "Column names: ['Emotions', 'Path']\n",
      "\n",
      "DataFrame shape before cleaning: (12161, 2)\n",
      "DataFrame shape after removing nulls: (12161, 2)\n",
      "Removed 0 rows with non-existent files\n",
      "\n",
      "Found 8 unique labels:\n",
      "  angry: 0\n",
      "  calm: 1\n",
      "  disgust: 2\n",
      "  fear: 3\n",
      "  happy: 4\n",
      "  neutral: 5\n",
      "  sad: 6\n",
      "  surprise: 7\n",
      "\n",
      "DataFrame after adding label_id:\n",
      "                                                path    label  label_id\n",
      "0  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...  neutral         5\n",
      "1  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...  neutral         5\n",
      "2  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...  neutral         5\n",
      "3  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...  neutral         5\n",
      "4  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...     calm         1\n",
      "\n",
      "Dataset created successfully!\n",
      "Dataset info: Dataset({\n",
      "    features: ['label', 'path', 'label_id'],\n",
      "    num_rows: 12161\n",
      "})\n",
      "Processor loaded successfully!\n",
      "\n",
      "Final dataset shape: 12161 samples\n",
      "Features: {'label': Value(dtype='string', id=None), 'path': Value(dtype='string', id=None), 'label_id': Value(dtype='int64', id=None)}\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🖥️ Using device: {device}\")\n",
    "\n",
    "MODEL_NAME = \"facebook/wav2vec2-base\"\n",
    "NUM_LABELS = 8  # Change this if needed\n",
    "\n",
    "# ===============================\n",
    "# 2. Load Processor and Model\n",
    "# ===============================\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"single_label_classification\"\n",
    ").to(device)"
=======
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\SpeechSentemintAnalysis\\data\\afterReadingDataSet.csv\")\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.head())\n",
    "print(\"\\nColumn names:\", df.columns.tolist())\n",
    "\n",
    "# Fix column names\n",
    "df = df.rename(columns={'Emotions': 'label', 'Path': 'path'})\n",
    "\n",
    "# Remove rows with null path or label\n",
    "print(f\"\\nDataFrame shape before cleaning: {df.shape}\")\n",
    "df = df.dropna(subset=[\"path\", \"label\"])\n",
    "print(f\"DataFrame shape after removing nulls: {df.shape}\")\n",
    "\n",
    "# Optional: Remove rows where path file doesn't exist\n",
    "df_before_file_check = len(df)\n",
    "df = df[df[\"path\"].apply(os.path.exists)]\n",
    "print(f\"Removed {df_before_file_check - len(df)} rows with non-existent files\")\n",
    "\n",
    "# Map emotion labels to integers\n",
    "label_list = sorted(df[\"label\"].unique())\n",
    "label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(f\"\\nFound {len(label_list)} unique labels:\")\n",
    "for label, idx in label2id.items():\n",
    "    print(f\"  {label}: {idx}\")\n",
    "\n",
    "# Add label_id column\n",
    "df[\"label_id\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "print(f\"\\nDataFrame after adding label_id:\")\n",
    "print(df[[\"path\", \"label\", \"label_id\"]].head())\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(f\"\\nDataset created successfully!\")\n",
    "print(f\"Dataset info: {dataset}\")\n",
    "\n",
    "# Load processor\n",
    "try:\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
    "    print(\"Processor loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading processor: {e}\")\n",
    "    print(\"Make sure transformers is installed: pip install transformers\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {len(dataset)} samples\")\n",
    "print(f\"Features: {dataset.features}\")\n"
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "84dcc835",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/teamspace/studios/this_studio/speechSentimentAnalysis/processed_data\")\n",
    "\n",
    "# Load parquet files\n",
    "train_df = pd.read_parquet(DATA_DIR / \"train_processed.parquet\")\n",
    "valid_df = pd.read_parquet(DATA_DIR / \"valid_processed.parquet\")\n",
    "test_df  = pd.read_parquet(DATA_DIR / \"test_processed.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 20,
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   "id": "e48f87bf",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(valid_df)\n",
    "test_dataset  = Dataset.from_pandas(test_df)\n",
    "datasets = {\n",
    "    \"train\": train_dataset,\n",
    "    \"valid\": valid_dataset,\n",
    "    \"test\": test_dataset\n",
    "}\n"
=======
    "\n",
    "# Set target sample rate\n",
    "target_sampling_rate = 16000\n",
    "\n",
    "def preprocess(example):\n",
    "    \"\"\"\n",
    "    Preprocess audio files and return the processed example\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        speech_array, sampling_rate = torchaudio.load(example['path'])\n",
    "        \n",
    "        # Resample if necessary\n",
    "        if sampling_rate != target_sampling_rate:\n",
    "            resampler = torchaudio.transforms.Resample(\n",
    "                orig_freq=sampling_rate, \n",
    "                new_freq=target_sampling_rate\n",
    "            )\n",
    "            speech_array = resampler(speech_array)\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if speech_array.shape[0] > 1:\n",
    "            speech_array = torch.mean(speech_array, dim=0, keepdim=True)\n",
    "        \n",
    "        # Process with Wav2Vec2 processor\n",
    "        inputs = processor(\n",
    "            speech_array.squeeze().numpy(), \n",
    "            sampling_rate=target_sampling_rate, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Extract input_values and convert to list for Arrow compatibility\n",
    "        input_values = inputs[\"input_values\"].squeeze().tolist()\n",
    "        \n",
    "        # Update example with processed data\n",
    "        example[\"input_values\"] = input_values\n",
    "        example[\"labels\"] = example[\"label_id\"]  # Use 'labels' for consistency with transformers\n",
    "        \n",
    "        return example\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {example['path']}: {e}\")\n",
    "        # Return None to indicate this example should be filtered out\n",
    "        return None\n",
    "\n",
    "def filter_failed_examples(example):\n",
    "    \"\"\"Filter function to remove None examples\"\"\"\n",
    "    return example is not None\n"
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 21,
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   "id": "d4c3e2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SPLITTING DATASET\n",
      "==================================================\n",
      "Train dataset size: 9728\n",
      "Validation dataset size: 1216\n",
      "Test dataset size: 1217\n",
      "\n",
      "==================================================\n",
      "PREPROCESSING DATASETS\n",
      "==================================================\n",
      "Processing training dataset...\n",
      "Processing train example 0/9728\n",
      "Processing train example 100/9728\n",
      "Processing train example 200/9728\n",
      "Processing train example 300/9728\n",
      "Processing train example 400/9728\n",
      "Processing train example 500/9728\n",
      "Processing train example 600/9728\n",
      "Processing train example 700/9728\n",
      "Processing train example 800/9728\n",
      "Processing train example 900/9728\n",
      "Processing train example 1000/9728\n",
      "Processing train example 1100/9728\n",
      "Processing train example 1200/9728\n",
      "Processing train example 1300/9728\n",
      "Processing train example 1400/9728\n",
      "Processing train example 1500/9728\n",
      "Processing train example 1600/9728\n",
      "Processing train example 1700/9728\n",
      "Processing train example 1800/9728\n",
      "Processing train example 1900/9728\n",
      "Processing train example 2000/9728\n",
      "Processing train example 2100/9728\n",
      "Processing train example 2200/9728\n",
      "Processing train example 2300/9728\n",
      "Processing train example 2400/9728\n",
      "Processing train example 2500/9728\n",
      "Processing train example 2600/9728\n",
      "Processing train example 2700/9728\n",
      "Processing train example 2800/9728\n",
      "Processing train example 2900/9728\n",
      "Processing train example 3000/9728\n",
      "Processing train example 3100/9728\n",
      "Processing train example 3200/9728\n",
      "Processing train example 3300/9728\n",
      "Processing train example 3400/9728\n",
      "Processing train example 3500/9728\n",
      "Processing train example 3600/9728\n",
      "Processing train example 3700/9728\n",
      "Processing train example 3800/9728\n",
      "Processing train example 3900/9728\n",
      "Processing train example 4000/9728\n",
      "Processing train example 4100/9728\n",
      "Processing train example 4200/9728\n",
      "Processing train example 4300/9728\n",
      "Processing train example 4400/9728\n",
      "Processing train example 4500/9728\n",
      "Processing train example 4600/9728\n",
      "Processing train example 4700/9728\n",
      "Processing train example 4800/9728\n",
      "Processing train example 4900/9728\n",
      "Processing train example 5000/9728\n",
      "Processing train example 5100/9728\n",
      "Processing train example 5200/9728\n",
      "Processing train example 5300/9728\n",
      "Processing train example 5400/9728\n",
      "Processing train example 5500/9728\n",
      "Processing train example 5600/9728\n",
      "Processing train example 5700/9728\n",
      "Processing train example 5800/9728\n",
      "Processing train example 5900/9728\n",
      "Processing train example 6000/9728\n",
      "Processing train example 6100/9728\n",
      "Processing train example 6200/9728\n",
      "Processing train example 6300/9728\n",
      "Processing train example 6400/9728\n",
      "Processing train example 6500/9728\n",
      "Processing train example 6600/9728\n",
      "Processing train example 6700/9728\n",
      "Processing train example 6800/9728\n",
      "Processing train example 6900/9728\n",
      "Processing train example 7000/9728\n",
      "Processing train example 7100/9728\n",
      "Processing train example 7200/9728\n",
      "Processing train example 7300/9728\n",
      "Processing train example 7400/9728\n",
      "Processing train example 7500/9728\n",
      "Processing train example 7600/9728\n",
      "Processing train example 7700/9728\n",
      "Processing train example 7800/9728\n",
      "Processing train example 7900/9728\n",
      "Processing train example 8000/9728\n",
      "Processing train example 8100/9728\n",
      "Processing train example 8200/9728\n",
      "Processing train example 8300/9728\n",
      "Processing train example 8400/9728\n",
      "Processing train example 8500/9728\n",
      "Processing train example 8600/9728\n",
      "Processing train example 8700/9728\n",
      "Processing train example 8800/9728\n",
      "Processing train example 8900/9728\n",
      "Processing train example 9000/9728\n",
      "Processing train example 9100/9728\n",
      "Processing train example 9200/9728\n",
      "Processing train example 9300/9728\n",
      "Processing train example 9400/9728\n",
      "Processing train example 9500/9728\n",
      "Processing train example 9600/9728\n",
      "Processing train example 9700/9728\n",
      "Successfully processed 9728/9728 training examples\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "from typing import List, Dict, Any\n",
    "\n",
    "# Step 1: Preprocessing function - DON'T pad here, just truncate\n",
    "def preprocess_example(example):\n",
    "    audio = example[\"input_values\"]\n",
    "    audio = np.array(audio, dtype=np.float32)\n",
    "\n",
    "    max_len = 16000 * 10  # 10 seconds\n",
    "    # Only truncate if too long, don't pad here\n",
    "    if len(audio) > max_len:\n",
    "        audio = audio[:max_len]\n",
    "    \n",
    "    return {\"input_values\": audio}  # Return original length\n",
    "\n",
    "# Step 2: Define memory-safe data collator - padding happens here during batch creation\n",
    "class DataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        input_values = [torch.tensor(f[\"input_values\"], dtype=torch.float32) for f in features]\n",
    "        labels = [f[\"labels\"] for f in features]\n",
    "        max_len = max(len(i) for i in input_values)\n",
    "        padded = [torch.cat([i, torch.zeros(max_len - len(i))]) if len(i) < max_len else i for i in input_values]\n",
    "        batch_inputs = torch.stack(padded)\n",
    "        return {\"input_values\": batch_inputs, \"labels\": torch.tensor(labels, dtype=torch.long)}\n",
    "\n",
    "collator = DataCollator(processor)\n",
    "\n",
    "\n",
    "# %%\n",
    "print(\"Preprocessing train_dataset...\")\n",
    "\n",
    "# Process in smaller chunks and clear cache frequently\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_example,\n",
    "    batched=False,\n",
    "    desc=\"Processing train\",\n",
    "    load_from_cache_file=False,\n",
    "    writer_batch_size=100  # Write to disk more frequently\n",
    ")\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "print(\"✅ Done with train_dataset\")\n",
    "\n",
    "\n",
    "# %%\n",
    "print(\"Preprocessing valid_dataset...\")\n",
    "\n",
    "valid_dataset = valid_dataset.map(\n",
    "    preprocess_example,\n",
    "    batched=False,\n",
    "    desc=\"Processing valid\",\n",
    "    load_from_cache_file=False,\n",
    "    writer_batch_size=100\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "print(\"✅ Done with valid_dataset\")\n",
    "\n",
    "print(\"Preprocessing test_dataset...\")\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    preprocess_example,\n",
    "    batched=False,\n",
    "    desc=\"Processing test\",\n",
    "    load_from_cache_file=False,\n",
    "    writer_batch_size=100\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "print(\"✅ Done with test_dataset\")\n"
=======
    "\n",
    "# Split dataset BEFORE preprocessing to avoid issues\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SPLITTING DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "train_testvalid = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "valid_test = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "train_dataset = train_testvalid['train']\n",
    "valid_dataset = valid_test['train']\n",
    "test_dataset = valid_test['test']\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(valid_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Preprocess datasets\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREPROCESSING DATASETS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Process train dataset\n",
    "print(\"Processing training dataset...\")\n",
    "train_processed = []\n",
    "for i, example in enumerate(train_dataset):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processing train example {i}/{len(train_dataset)}\")\n",
    "    \n",
    "    processed = preprocess(example)\n",
    "    if processed is not None:\n",
    "        train_processed.append(processed)\n",
    "\n",
    "print(f\"Successfully processed {len(train_processed)}/{len(train_dataset)} training examples\")\n"
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 22,
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   "id": "4f51050b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation dataset...\n",
      "Processing valid example 0/1216\n",
      "Processing valid example 50/1216\n",
      "Processing valid example 100/1216\n",
      "Processing valid example 150/1216\n",
      "Processing valid example 200/1216\n",
      "Processing valid example 250/1216\n",
      "Processing valid example 300/1216\n",
      "Processing valid example 350/1216\n",
      "Processing valid example 400/1216\n",
      "Processing valid example 450/1216\n",
      "Processing valid example 500/1216\n",
      "Processing valid example 550/1216\n",
      "Processing valid example 600/1216\n",
      "Processing valid example 650/1216\n",
      "Processing valid example 700/1216\n",
      "Processing valid example 750/1216\n",
      "Processing valid example 800/1216\n",
      "Processing valid example 850/1216\n",
      "Processing valid example 900/1216\n",
      "Processing valid example 950/1216\n",
      "Processing valid example 1000/1216\n",
      "Processing valid example 1050/1216\n",
      "Processing valid example 1100/1216\n",
      "Processing valid example 1150/1216\n",
      "Processing valid example 1200/1216\n",
      "Successfully processed 1216/1216 validation examples\n",
      "Processing test dataset...\n",
      "Processing test example 0/1217\n",
      "Processing test example 50/1217\n",
      "Processing test example 100/1217\n",
      "Processing test example 150/1217\n",
      "Processing test example 200/1217\n",
      "Processing test example 250/1217\n",
      "Processing test example 300/1217\n",
      "Processing test example 350/1217\n",
      "Processing test example 400/1217\n",
      "Processing test example 450/1217\n",
      "Processing test example 500/1217\n",
      "Processing test example 550/1217\n",
      "Processing test example 600/1217\n",
      "Processing test example 650/1217\n",
      "Processing test example 700/1217\n",
      "Processing test example 750/1217\n",
      "Processing test example 800/1217\n",
      "Processing test example 850/1217\n",
      "Processing test example 900/1217\n",
      "Processing test example 950/1217\n",
      "Processing test example 1000/1217\n",
      "Processing test example 1050/1217\n",
      "Processing test example 1100/1217\n",
      "Processing test example 1150/1217\n",
      "Processing test example 1200/1217\n",
      "Successfully processed 1217/1217 test examples\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Metric\n",
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    return {\"accuracy\": (preds == pred.label_ids).mean()}\n",
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-emotion\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    dataloader_num_workers=0,  # Reduce memory overhead\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
=======
    "\n",
    "# Process validation dataset\n",
    "print(\"Processing validation dataset...\")\n",
    "valid_processed = []\n",
    "for i, example in enumerate(valid_dataset):\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Processing valid example {i}/{len(valid_dataset)}\")\n",
    "    \n",
    "    processed = preprocess(example)\n",
    "    if processed is not None:\n",
    "        valid_processed.append(processed)\n",
    "\n",
    "print(f\"Successfully processed {len(valid_processed)}/{len(valid_dataset)} validation examples\")\n",
    "\n",
    "# Process test dataset\n",
    "print(\"Processing test dataset...\")\n",
    "test_processed = []\n",
    "for i, example in enumerate(test_dataset):\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Processing test example {i}/{len(test_dataset)}\")\n",
    "    \n",
    "    processed = preprocess(example)\n",
    "    if processed is not None:\n",
    "        test_processed.append(processed)\n",
    "\n",
    "print(f\"Successfully processed {len(test_processed)}/{len(test_dataset)} test examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db405af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 MEMORY-EFFICIENT SAVING (without Dataset conversion)\n",
      "============================================================\n",
      "💾 Saving processed data as pickle files...\n",
      "Saving 9728 training samples...\n",
      "✅ Saved training data to pickle\n",
      "Saving 1216 validation samples...\n",
      "✅ Saved validation data to pickle\n",
      "Saving 1217 test samples...\n",
      "✅ Saved test data to pickle\n",
      "✅ Saved label mappings to: C:\\Users\\asus\\Desktop\\SpeechSentemintAnalysis\\processed_data\\label_mappings.json\n",
      "✅ Saved processing statistics to: C:\\Users\\asus\\Desktop\\SpeechSentemintAnalysis\\processed_data\\processing_stats.json\n",
      "\n",
      "📁 All files saved to directory: C:\\Users\\asus\\Desktop\\SpeechSentemintAnalysis\\processed_data\n",
      "\n",
      "============================================================\n",
      "🎯 DATA SAVED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "🔄 Converting to Dataset format in batches...\n",
      "Converting 9728 samples to Dataset...\n",
      "✅ Test batch successful\n",
      "⚠️  Dataset too large (9728 samples), saving as pickle only\n",
      "Converting 1216 samples to Dataset...\n",
      "✅ Test batch successful\n",
      "⚠️  Dataset too large (1216 samples), saving as pickle only\n",
      "Converting 1217 samples to Dataset...\n",
      "✅ Test batch successful\n",
      "⚠️  Dataset too large (1217 samples), saving as pickle only\n",
      "\n",
      "🧹 Clearing train_processed from memory...\n",
      "🧹 Clearing valid_processed from memory...\n",
      "🧹 Clearing test_processed from memory...\n",
      "🧹 Memory cleared\n",
      "\n",
      "📊 SAVE SUMMARY:\n",
      "✅ Pickle files: All datasets saved successfully\n",
      "✅ Dataset objects: 0/3 saved successfully\n",
      "✅ Label mappings: Saved\n",
      "✅ Statistics: Saved\n",
      "\n",
      "🚀 NEXT STEPS:\n",
      "⚠️  Some datasets too large for Arrow format\n",
      "💡 Use the batch loading script below for training\n",
      "\n",
      "📁 Files in C:\\Users\\asus\\Desktop\\SpeechSentemintAnalysis\\processed_data:\n",
      "   📄 label_mappings.json (0.0 MB)\n",
      "   📄 processing_stats.json (0.0 MB)\n",
      "   📄 test_processed.pkl (438.6 MB)\n",
      "   📄 train_processed.pkl (3496.2 MB)\n",
      "   📄 valid_processed.pkl (442.3 MB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import gc\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"🔄 MEMORY-EFFICIENT SAVING (without Dataset conversion)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "save_dir = r\"C:\\Users\\asus\\Desktop\\SpeechSentemintAnalysis\\processed_data\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save processed lists directly as pickle files first (more memory efficient)\n",
    "print(\"💾 Saving processed data as pickle files...\")\n",
    "\n",
    "if 'train_processed' in locals() and train_processed:\n",
    "    print(f\"Saving {len(train_processed)} training samples...\")\n",
    "    with open(os.path.join(save_dir, \"train_processed.pkl\"), 'wb') as f:\n",
    "        pickle.dump(train_processed, f)\n",
    "    print(f\"✅ Saved training data to pickle\")\n",
    "\n",
    "if 'valid_processed' in locals() and valid_processed:\n",
    "    print(f\"Saving {len(valid_processed)} validation samples...\")\n",
    "    with open(os.path.join(save_dir, \"valid_processed.pkl\"), 'wb') as f:\n",
    "        pickle.dump(valid_processed, f)\n",
    "    print(f\"✅ Saved validation data to pickle\")\n",
    "\n",
    "if 'test_processed' in locals() and test_processed:\n",
    "    print(f\"Saving {len(test_processed)} test samples...\")\n",
    "    with open(os.path.join(save_dir, \"test_processed.pkl\"), 'wb') as f:\n",
    "        pickle.dump(test_processed, f)\n",
    "    print(f\"✅ Saved test data to pickle\")\n",
    "\n",
    "# Save label mappings\n",
    "if 'label2id' in locals():\n",
    "    mappings = {\n",
    "        \"label2id\": label2id,\n",
    "        \"id2label\": id2label,\n",
    "        \"label_list\": label_list,\n",
    "        \"num_labels\": len(label_list)\n",
    "    }\n",
    "    \n",
    "    mappings_path = os.path.join(save_dir, \"label_mappings.json\")\n",
    "    with open(mappings_path, 'w') as f:\n",
    "        json.dump(mappings, f, indent=2)\n",
    "    print(f\"✅ Saved label mappings to: {mappings_path}\")\n",
    "\n",
    "# Save processing statistics\n",
    "if 'dataset' in locals():\n",
    "    stats = {\n",
    "        \"train_processed_size\": len(train_processed) if 'train_processed' in locals() and train_processed else 0,\n",
    "        \"valid_processed_size\": len(valid_processed) if 'valid_processed' in locals() and valid_processed else 0,\n",
    "        \"test_processed_size\": len(test_processed) if 'test_processed' in locals() and test_processed else 0,\n",
    "        \"target_sampling_rate\": target_sampling_rate if 'target_sampling_rate' in locals() else 16000,\n",
    "        \"processor_model\": \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "    }\n",
    "    \n",
    "    stats_path = os.path.join(save_dir, \"processing_stats.json\")\n",
    "    with open(stats_path, 'w') as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    print(f\"✅ Saved processing statistics to: {stats_path}\")\n",
    "\n",
    "print(f\"\\n📁 All files saved to directory: {save_dir}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 DATA SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Now try to create and save Dataset objects in batches to avoid memory issues\n",
    "print(\"\\n🔄 Converting to Dataset format in batches...\")\n",
    "\n",
    "def save_dataset_in_batches(processed_data, save_path, batch_size=100):\n",
    "    \"\"\"Save large datasets in smaller batches to avoid memory issues\"\"\"\n",
    "    if not processed_data:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        print(f\"Converting {len(processed_data)} samples to Dataset...\")\n",
    "        \n",
    "        # Try smaller batch first to test memory\n",
    "        test_batch = processed_data[:min(10, len(processed_data))]\n",
    "        test_dataset = Dataset.from_list(test_batch)\n",
    "        print(f\"✅ Test batch successful\")\n",
    "        \n",
    "        # If test works, try full dataset\n",
    "        if len(processed_data) <= batch_size:\n",
    "            # Small enough to do all at once\n",
    "            dataset = Dataset.from_list(processed_data)\n",
    "            dataset.save_to_disk(save_path)\n",
    "            print(f\"✅ Saved complete dataset to {save_path}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"⚠️  Dataset too large ({len(processed_data)} samples), saving as pickle only\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Memory error during Dataset conversion: {e}\")\n",
    "        print(f\"💡 Data saved as pickle files - use batch loading script instead\")\n",
    "        return False\n",
    "\n",
    "# Try to save as Dataset objects (will fallback to pickle if memory issues)\n",
    "success_count = 0\n",
    "\n",
    "if 'train_processed' in locals() and train_processed:\n",
    "    train_save_path = os.path.join(save_dir, \"train_dataset\")\n",
    "    if save_dataset_in_batches(train_processed, train_save_path):\n",
    "        success_count += 1\n",
    "\n",
    "if 'valid_processed' in locals() and valid_processed:\n",
    "    valid_save_path = os.path.join(save_dir, \"valid_dataset\")\n",
    "    if save_dataset_in_batches(valid_processed, valid_save_path):\n",
    "        success_count += 1\n",
    "\n",
    "if 'test_processed' in locals() and test_processed:\n",
    "    test_save_path = os.path.join(save_dir, \"test_dataset\")\n",
    "    if save_dataset_in_batches(test_processed, test_save_path):\n",
    "        success_count += 1\n",
    "\n",
    "# Clear memory\n",
    "if 'train_processed' in locals():\n",
    "    print(f\"\\n🧹 Clearing train_processed from memory...\")\n",
    "    del train_processed\n",
    "if 'valid_processed' in locals():\n",
    "    print(f\"🧹 Clearing valid_processed from memory...\")\n",
    "    del valid_processed\n",
    "if 'test_processed' in locals():\n",
    "    print(f\"🧹 Clearing test_processed from memory...\")\n",
    "    del test_processed\n",
    "\n",
    "gc.collect()\n",
    "print(f\"🧹 Memory cleared\")\n",
    "\n",
    "print(f\"\\n📊 SAVE SUMMARY:\")\n",
    "print(f\"✅ Pickle files: All datasets saved successfully\")\n",
    "print(f\"✅ Dataset objects: {success_count}/3 saved successfully\")\n",
    "print(f\"✅ Label mappings: Saved\")\n",
    "print(f\"✅ Statistics: Saved\")\n",
    "\n",
    "print(f\"\\n🚀 NEXT STEPS:\")\n",
    "if success_count == 3:\n",
    "    print(f\"✅ All datasets saved in both formats - use the quick load script!\")\n",
    "else:\n",
    "    print(f\"⚠️  Some datasets too large for Arrow format\")\n",
    "    print(f\"💡 Use the batch loading script below for training\")\n",
    "\n",
    "print(f\"\\n📁 Files in {save_dir}:\")\n",
    "for file in os.listdir(save_dir):\n",
    "    file_path = os.path.join(save_dir, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size_mb = os.path.getsize(file_path) / (1024*1024)\n",
    "        print(f\"   📄 {file} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"   📁 {file}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c409335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display sample from processed dataset\n",
    "if train_processed:\n",
    "    print(f\"\\nSample from processed training data:\")\n",
    "    sample = train_dataset[0]\n",
    "    print(f\"Keys: {sample.keys()}\")\n",
    "    print(f\"Input values length: {len(sample['input_values'])}\")\n",
    "    print(f\"Label: {sample['labels']}\")\n",
    "    print(f\"Original label: {sample['label']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUICK LOAD CODE FOR NEXT TIME:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\"\"\n",
    "# To load your processed datasets next time:\n",
    "from datasets import load_from_disk\n",
    "import json\n",
    "\n",
    "save_dir = r\"{save_dir}\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = load_from_disk(os.path.join(save_dir, \"train_dataset\"))\n",
    "valid_dataset = load_from_disk(os.path.join(save_dir, \"valid_dataset\"))\n",
    "test_dataset = load_from_disk(os.path.join(save_dir, \"test_dataset\"))\n",
    "\n",
    "# Load label mappings\n",
    "with open(os.path.join(save_dir, \"label_mappings.json\"), 'r') as f:\n",
    "    mappings = json.load(f)\n",
    "    label2id = mappings[\"label2id\"]\n",
    "    id2label = mappings[\"id2label\"]\n",
    "    num_labels = mappings[\"num_labels\"]\n",
    "\n",
    "print(f\"Loaded datasets - Train: {{len(train_dataset)}}, Valid: {{len(valid_dataset)}}, Test: {{len(test_dataset)}}\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114cb2e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgc\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Wav2Vec2Processor\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBatchDataLoader\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\__init__.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m3.6.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:56\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfsspec\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpa\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\__init__.py:59\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[32m     61\u001b[39m     ArrowDtype,\n\u001b[32m     62\u001b[39m     Int8Dtype,\n\u001b[32m     63\u001b[39m     Int16Dtype,\n\u001b[32m     64\u001b[39m     Int32Dtype,\n\u001b[32m     65\u001b[39m     Int64Dtype,\n\u001b[32m     66\u001b[39m     UInt8Dtype,\n\u001b[32m     67\u001b[39m     UInt16Dtype,\n\u001b[32m     68\u001b[39m     UInt32Dtype,\n\u001b[32m     69\u001b[39m     UInt64Dtype,\n\u001b[32m     70\u001b[39m     Float32Dtype,\n\u001b[32m     71\u001b[39m     Float64Dtype,\n\u001b[32m     72\u001b[39m     CategoricalDtype,\n\u001b[32m     73\u001b[39m     PeriodDtype,\n\u001b[32m     74\u001b[39m     IntervalDtype,\n\u001b[32m     75\u001b[39m     DatetimeTZDtype,\n\u001b[32m     76\u001b[39m     StringDtype,\n\u001b[32m     77\u001b[39m     BooleanDtype,\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[32m     79\u001b[39m     NA,\n\u001b[32m     80\u001b[39m     isna,\n\u001b[32m     81\u001b[39m     isnull,\n\u001b[32m     82\u001b[39m     notna,\n\u001b[32m     83\u001b[39m     notnull,\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[32m     85\u001b[39m     Index,\n\u001b[32m     86\u001b[39m     CategoricalIndex,\n\u001b[32m     87\u001b[39m     RangeIndex,\n\u001b[32m     88\u001b[39m     MultiIndex,\n\u001b[32m     89\u001b[39m     IntervalIndex,\n\u001b[32m     90\u001b[39m     TimedeltaIndex,\n\u001b[32m     91\u001b[39m     DatetimeIndex,\n\u001b[32m     92\u001b[39m     PeriodIndex,\n\u001b[32m     93\u001b[39m     IndexSlice,\n\u001b[32m     94\u001b[39m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[32m     95\u001b[39m     NaT,\n\u001b[32m     96\u001b[39m     Period,\n\u001b[32m     97\u001b[39m     period_range,\n\u001b[32m     98\u001b[39m     Timedelta,\n\u001b[32m     99\u001b[39m     timedelta_range,\n\u001b[32m    100\u001b[39m     Timestamp,\n\u001b[32m    101\u001b[39m     date_range,\n\u001b[32m    102\u001b[39m     bdate_range,\n\u001b[32m    103\u001b[39m     Interval,\n\u001b[32m    104\u001b[39m     interval_range,\n\u001b[32m    105\u001b[39m     DateOffset,\n\u001b[32m    106\u001b[39m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[32m    107\u001b[39m     to_numeric,\n\u001b[32m    108\u001b[39m     to_datetime,\n\u001b[32m    109\u001b[39m     to_timedelta,\n\u001b[32m    110\u001b[39m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[32m    111\u001b[39m     Flags,\n\u001b[32m    112\u001b[39m     Grouper,\n\u001b[32m    113\u001b[39m     factorize,\n\u001b[32m    114\u001b[39m     unique,\n\u001b[32m    115\u001b[39m     value_counts,\n\u001b[32m    116\u001b[39m     NamedAgg,\n\u001b[32m    117\u001b[39m     array,\n\u001b[32m    118\u001b[39m     Categorical,\n\u001b[32m    119\u001b[39m     set_eng_float_format,\n\u001b[32m    120\u001b[39m     Series,\n\u001b[32m    121\u001b[39m     DataFrame,\n\u001b[32m    122\u001b[39m )\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtseries\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     NaT,\n\u001b[32m      3\u001b[39m     Period,\n\u001b[32m      4\u001b[39m     Timedelta,\n\u001b[32m      5\u001b[39m     Timestamp,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     ArrowDtype,\n\u001b[32m     11\u001b[39m     CategoricalDtype,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     PeriodDtype,\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\__init__.py:18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401,E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minterval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     NaT,\n\u001b[32m     21\u001b[39m     NaTType,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     iNaT,\n\u001b[32m     27\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32minterval.pyx:1\u001b[39m, in \u001b[36minit pandas._libs.interval\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import gc\n",
    "from datasets import Dataset\n",
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "class BatchDataLoader:\n",
    "    \"\"\"Memory-efficient loader for large processed datasets\"\"\"\n",
    "    \n",
    "    def __init__(self, save_dir):\n",
    "        self.save_dir = save_dir\n",
    "        self.label_mappings = None\n",
    "        self.stats = None\n",
    "        self.processor = None\n",
    "        \n",
    "    def load_metadata(self):\n",
    "        \"\"\"Load label mappings and statistics\"\"\"\n",
    "        print(\"📋 Loading metadata...\")\n",
    "        \n",
    "        # Load label mappings\n",
    "        mappings_path = os.path.join(self.save_dir, \"label_mappings.json\")\n",
    "        if os.path.exists(mappings_path):\n",
    "            with open(mappings_path, 'r') as f:\n",
    "                self.label_mappings = json.load(f)\n",
    "            print(f\"✅ Loaded {self.label_mappings['num_labels']} labels\")\n",
    "        \n",
    "        # Load stats\n",
    "        stats_path = os.path.join(self.save_dir, \"processing_stats.json\")\n",
    "        if os.path.exists(stats_path):\n",
    "            with open(stats_path, 'r') as f:\n",
    "                self.stats = json.load(f)\n",
    "            print(f\"✅ Loaded processing statistics\")\n",
    "        \n",
    "        # Load processor\n",
    "        try:\n",
    "            self.processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
    "            print(f\"✅ Loaded processor\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Could not load processor: {e}\")\n",
    "    \n",
    "    def try_load_dataset(self, dataset_name):\n",
    "        \"\"\"Try to load dataset, fallback to pickle if needed\"\"\"\n",
    "        dataset_path = os.path.join(self.save_dir, f\"{dataset_name}_dataset\")\n",
    "        pickle_path = os.path.join(self.save_dir, f\"{dataset_name}_processed.pkl\")\n",
    "        \n",
    "        # Try loading as Dataset first\n",
    "        if os.path.exists(dataset_path):\n",
    "            try:\n",
    "                dataset = Dataset.load_from_disk(dataset_path)\n",
    "                print(f\"✅ Loaded {dataset_name} dataset ({len(dataset)} samples)\")\n",
    "                return dataset\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Could not load {dataset_name} dataset: {e}\")\n",
    "        \n",
    "        # Fallback to pickle\n",
    "        if os.path.exists(pickle_path):\n",
    "            try:\n",
    "                print(f\"🔄 Loading {dataset_name} from pickle...\")\n",
    "                with open(pickle_path, 'rb') as f:\n",
    "                    processed_data = pickle.load(f)\n",
    "                \n",
    "                # Try to convert to Dataset in smaller batches\n",
    "                return self.pickle_to_dataset_batched(processed_data, dataset_name)\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Could not load {dataset_name} pickle: {e}\")\n",
    "                return None\n",
    "        \n",
    "        print(f\"❌ No data found for {dataset_name}\")\n",
    "        return None\n",
    "    \n",
    "    def pickle_to_dataset_batched(self, processed_data, name, batch_size=50):\n",
    "        \"\"\"Convert pickle data to Dataset in batches\"\"\"\n",
    "        if not processed_data:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Try small batch first\n",
    "            test_size = min(10, len(processed_data))\n",
    "            test_batch = processed_data[:test_size]\n",
    "            test_dataset = Dataset.from_list(test_batch)\n",
    "            print(f\"✅ Test conversion successful for {name}\")\n",
    "            \n",
    "            # If data is small enough, convert all at once\n",
    "            if len(processed_data) <= batch_size:\n",
    "                dataset = Dataset.from_list(processed_data)\n",
    "                print(f\"✅ Converted all {len(processed_data)} {name} samples to Dataset\")\n",
    "                return dataset\n",
    "            else:\n",
    "                # For larger datasets, you might want to return a custom iterator\n",
    "                print(f\"⚠️  {name} dataset large ({len(processed_data)} samples)\")\n",
    "                print(f\"💡 Consider using smaller batches during training\")\n",
    "                \n",
    "                # Still try to convert, but with more aggressive memory management\n",
    "                gc.collect()\n",
    "                dataset = Dataset.from_list(processed_data)\n",
    "                print(f\"✅ Converted {len(processed_data)} {name} samples (may use lots of memory)\")\n",
    "                return dataset\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to convert {name} pickle to Dataset: {e}\")\n",
    "            print(f\"💡 You may need to use the raw pickle data with custom data loaders\")\n",
    "            return None\n",
    "    \n",
    "    def load_all(self):\n",
    "        \"\"\"Load all datasets and metadata\"\"\"\n",
    "        print(\"🚀 Loading all processed data...\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        self.load_metadata()\n",
    "        \n",
    "        # Try to load datasets\n",
    "        train_dataset = self.try_load_dataset(\"train\")\n",
    "        valid_dataset = self.try_load_dataset(\"valid\") \n",
    "        test_dataset = self.try_load_dataset(\"test\")\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\n📊 LOADING SUMMARY:\")\n",
    "        print(\"=\"*50)\n",
    "        datasets = {\n",
    "            \"train\": train_dataset,\n",
    "            \"valid\": valid_dataset, \n",
    "            \"test\": test_dataset\n",
    "        }\n",
    "        \n",
    "        for name, dataset in datasets.items():\n",
    "            if dataset is not None:\n",
    "                print(f\"✅ {name.capitalize()}: {len(dataset)} samples\")\n",
    "            else:\n",
    "                print(f\"❌ {name.capitalize()}: Failed to load\")\n",
    "        \n",
    "        print(f\"\\n🎯 Available variables:\")\n",
    "        if self.label_mappings:\n",
    "            print(f\"   - label2id, id2label, num_labels\")\n",
    "        if self.processor:\n",
    "            print(f\"   - processor\")\n",
    "        \n",
    "        return {\n",
    "            'train_dataset': train_dataset,\n",
    "            'valid_dataset': valid_dataset,\n",
    "            'test_dataset': test_dataset,\n",
    "            'label2id': self.label_mappings.get('label2id') if self.label_mappings else None,\n",
    "            'id2label': self.label_mappings.get('id2label') if self.label_mappings else None,\n",
    "            'num_labels': self.label_mappings.get('num_labels') if self.label_mappings else None,\n",
    "            'processor': self.processor,\n",
    "            'stats': self.stats\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "save_dir = r\"C:\\Users\\asus\\Desktop\\SpeechSentemintAnalysis\\processed_data\"\n",
    "\n",
    "# Check if save directory exists\n",
    "if os.path.exists(save_dir):\n",
    "    loader = BatchDataLoader(save_dir)\n",
    "    results = loader.load_all()\n",
    "    \n",
    "    # Extract variables for easy access\n",
    "    train_dataset = results['train_dataset']\n",
    "    valid_dataset = results['valid_dataset']\n",
    "    test_dataset = results['test_dataset']\n",
    "    label2id = results['label2id']\n",
    "    id2label = results['id2label']\n",
    "    num_labels = results['num_labels']\n",
    "    processor = results['processor']\n",
    "    \n",
    "    print(f\"\\n🎉 Data loading complete!\")\n",
    "    print(f\"Ready for model training with: train_dataset, valid_dataset, test_dataset\")\n",
    "    \n",
    "    # Display sample if available\n",
    "    if train_dataset and len(train_dataset) > 0:\n",
    "        print(f\"\\n🔍 Sample from training data:\")\n",
    "        sample = train_dataset[0]\n",
    "        print(f\"   - Keys: {list(sample.keys())}\")\n",
    "        print(f\"   - Input length: {len(sample['input_values'])}\")\n",
    "        if 'labels' in sample:\n",
    "            print(f\"   - Label: {sample['labels']} ({id2label[sample['labels']] if id2label else 'unknown'})\")\n",
    "        \n",
    "else:\n",
    "    print(f\"❌ Save directory not found: {save_dir}\")\n",
    "    print(f\"Please run the processing script first.\")"
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "lightning",
=======
   "display_name": "Python 3",
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.11.13"
=======
   "version": "3.11.9"
>>>>>>> 19d1d1dc97eb81e1d782bd8f64bd343fbf3dbc28
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
