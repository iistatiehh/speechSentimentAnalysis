{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b1ff5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchaudio\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset, Dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\__init__.py:59\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[32m     61\u001b[39m     ArrowDtype,\n\u001b[32m     62\u001b[39m     Int8Dtype,\n\u001b[32m     63\u001b[39m     Int16Dtype,\n\u001b[32m     64\u001b[39m     Int32Dtype,\n\u001b[32m     65\u001b[39m     Int64Dtype,\n\u001b[32m     66\u001b[39m     UInt8Dtype,\n\u001b[32m     67\u001b[39m     UInt16Dtype,\n\u001b[32m     68\u001b[39m     UInt32Dtype,\n\u001b[32m     69\u001b[39m     UInt64Dtype,\n\u001b[32m     70\u001b[39m     Float32Dtype,\n\u001b[32m     71\u001b[39m     Float64Dtype,\n\u001b[32m     72\u001b[39m     CategoricalDtype,\n\u001b[32m     73\u001b[39m     PeriodDtype,\n\u001b[32m     74\u001b[39m     IntervalDtype,\n\u001b[32m     75\u001b[39m     DatetimeTZDtype,\n\u001b[32m     76\u001b[39m     StringDtype,\n\u001b[32m     77\u001b[39m     BooleanDtype,\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[32m     79\u001b[39m     NA,\n\u001b[32m     80\u001b[39m     isna,\n\u001b[32m     81\u001b[39m     isnull,\n\u001b[32m     82\u001b[39m     notna,\n\u001b[32m     83\u001b[39m     notnull,\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[32m     85\u001b[39m     Index,\n\u001b[32m     86\u001b[39m     CategoricalIndex,\n\u001b[32m     87\u001b[39m     RangeIndex,\n\u001b[32m     88\u001b[39m     MultiIndex,\n\u001b[32m     89\u001b[39m     IntervalIndex,\n\u001b[32m     90\u001b[39m     TimedeltaIndex,\n\u001b[32m     91\u001b[39m     DatetimeIndex,\n\u001b[32m     92\u001b[39m     PeriodIndex,\n\u001b[32m     93\u001b[39m     IndexSlice,\n\u001b[32m     94\u001b[39m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[32m     95\u001b[39m     NaT,\n\u001b[32m     96\u001b[39m     Period,\n\u001b[32m     97\u001b[39m     period_range,\n\u001b[32m     98\u001b[39m     Timedelta,\n\u001b[32m     99\u001b[39m     timedelta_range,\n\u001b[32m    100\u001b[39m     Timestamp,\n\u001b[32m    101\u001b[39m     date_range,\n\u001b[32m    102\u001b[39m     bdate_range,\n\u001b[32m    103\u001b[39m     Interval,\n\u001b[32m    104\u001b[39m     interval_range,\n\u001b[32m    105\u001b[39m     DateOffset,\n\u001b[32m    106\u001b[39m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[32m    107\u001b[39m     to_numeric,\n\u001b[32m    108\u001b[39m     to_datetime,\n\u001b[32m    109\u001b[39m     to_timedelta,\n\u001b[32m    110\u001b[39m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[32m    111\u001b[39m     Flags,\n\u001b[32m    112\u001b[39m     Grouper,\n\u001b[32m    113\u001b[39m     factorize,\n\u001b[32m    114\u001b[39m     unique,\n\u001b[32m    115\u001b[39m     value_counts,\n\u001b[32m    116\u001b[39m     NamedAgg,\n\u001b[32m    117\u001b[39m     array,\n\u001b[32m    118\u001b[39m     Categorical,\n\u001b[32m    119\u001b[39m     set_eng_float_format,\n\u001b[32m    120\u001b[39m     Series,\n\u001b[32m    121\u001b[39m     DataFrame,\n\u001b[32m    122\u001b[39m )\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtseries\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     NaT,\n\u001b[32m      3\u001b[39m     Period,\n\u001b[32m      4\u001b[39m     Timedelta,\n\u001b[32m      5\u001b[39m     Timestamp,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     ArrowDtype,\n\u001b[32m     11\u001b[39m     CategoricalDtype,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     PeriodDtype,\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\__init__.py:18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401,E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minterval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     NaT,\n\u001b[32m     21\u001b[39m     NaTType,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     iNaT,\n\u001b[32m     27\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32minterval.pyx:1\u001b[39m, in \u001b[36minit pandas._libs.interval\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from datasets import Dataset\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification,DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "import gc\n",
    "import numpy as np\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fd1dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "  Emotions                                               Path\n",
      "0  neutral  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...\n",
      "1  neutral  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...\n",
      "2  neutral  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...\n",
      "3  neutral  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...\n",
      "4     calm  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...\n",
      "\n",
      "Column names: ['Emotions', 'Path']\n",
      "\n",
      "DataFrame shape before cleaning: (12161, 2)\n",
      "DataFrame shape after removing nulls: (12161, 2)\n",
      "Removed 0 rows with non-existent files\n",
      "\n",
      "Found 8 unique labels:\n",
      "  angry: 0\n",
      "  calm: 1\n",
      "  disgust: 2\n",
      "  fear: 3\n",
      "  happy: 4\n",
      "  neutral: 5\n",
      "  sad: 6\n",
      "  surprise: 7\n",
      "\n",
      "DataFrame after adding label_id:\n",
      "                                                path    label  label_id\n",
      "0  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...  neutral         5\n",
      "1  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...  neutral         5\n",
      "2  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...  neutral         5\n",
      "3  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...  neutral         5\n",
      "4  C:\\Users\\asus\\Desktop\\Speech sentiment\\speechS...     calm         1\n",
      "\n",
      "Dataset created successfully!\n",
      "Dataset info: Dataset({\n",
      "    features: ['label', 'path', 'label_id'],\n",
      "    num_rows: 12161\n",
      "})\n",
      "Processor loaded successfully!\n",
      "\n",
      "Final dataset shape: 12161 samples\n",
      "Features: {'label': Value(dtype='string', id=None), 'path': Value(dtype='string', id=None), 'label_id': Value(dtype='int64', id=None)}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "MODEL_NAME = \"facebook/wav2vec2-base\"\n",
    "NUM_LABELS = 8  # Change this if needed\n",
    "\n",
    "# ===============================\n",
    "# 2. Load Processor and Model\n",
    "# ===============================\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"single_label_classification\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dcc835",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/teamspace/studios/this_studio/speechSentimentAnalysis/processed_data\")\n",
    "\n",
    "# Load parquet files\n",
    "train_df = pd.read_parquet(DATA_DIR / \"train_processed.parquet\")\n",
    "valid_df = pd.read_parquet(DATA_DIR / \"valid_processed.parquet\")\n",
    "test_df  = pd.read_parquet(DATA_DIR / \"test_processed.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(valid_df)\n",
    "test_dataset  = Dataset.from_pandas(test_df)\n",
    "datasets = {\n",
    "    \"train\": train_dataset,\n",
    "    \"valid\": valid_dataset,\n",
    "    \"test\": test_dataset\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c3e2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SPLITTING DATASET\n",
      "==================================================\n",
      "Train dataset size: 9728\n",
      "Validation dataset size: 1216\n",
      "Test dataset size: 1217\n",
      "\n",
      "==================================================\n",
      "PREPROCESSING DATASETS\n",
      "==================================================\n",
      "Processing training dataset...\n",
      "Processing train example 0/9728\n",
      "Processing train example 100/9728\n",
      "Processing train example 200/9728\n",
      "Processing train example 300/9728\n",
      "Processing train example 400/9728\n",
      "Processing train example 500/9728\n",
      "Processing train example 600/9728\n",
      "Processing train example 700/9728\n",
      "Processing train example 800/9728\n",
      "Processing train example 900/9728\n",
      "Processing train example 1000/9728\n",
      "Processing train example 1100/9728\n",
      "Processing train example 1200/9728\n",
      "Processing train example 1300/9728\n",
      "Processing train example 1400/9728\n",
      "Processing train example 1500/9728\n",
      "Processing train example 1600/9728\n",
      "Processing train example 1700/9728\n",
      "Processing train example 1800/9728\n",
      "Processing train example 1900/9728\n",
      "Processing train example 2000/9728\n",
      "Processing train example 2100/9728\n",
      "Processing train example 2200/9728\n",
      "Processing train example 2300/9728\n",
      "Processing train example 2400/9728\n",
      "Processing train example 2500/9728\n",
      "Processing train example 2600/9728\n",
      "Processing train example 2700/9728\n",
      "Processing train example 2800/9728\n",
      "Processing train example 2900/9728\n",
      "Processing train example 3000/9728\n",
      "Processing train example 3100/9728\n",
      "Processing train example 3200/9728\n",
      "Processing train example 3300/9728\n",
      "Processing train example 3400/9728\n",
      "Processing train example 3500/9728\n",
      "Processing train example 3600/9728\n",
      "Processing train example 3700/9728\n",
      "Processing train example 3800/9728\n",
      "Processing train example 3900/9728\n",
      "Processing train example 4000/9728\n",
      "Processing train example 4100/9728\n",
      "Processing train example 4200/9728\n",
      "Processing train example 4300/9728\n",
      "Processing train example 4400/9728\n",
      "Processing train example 4500/9728\n",
      "Processing train example 4600/9728\n",
      "Processing train example 4700/9728\n",
      "Processing train example 4800/9728\n",
      "Processing train example 4900/9728\n",
      "Processing train example 5000/9728\n",
      "Processing train example 5100/9728\n",
      "Processing train example 5200/9728\n",
      "Processing train example 5300/9728\n",
      "Processing train example 5400/9728\n",
      "Processing train example 5500/9728\n",
      "Processing train example 5600/9728\n",
      "Processing train example 5700/9728\n",
      "Processing train example 5800/9728\n",
      "Processing train example 5900/9728\n",
      "Processing train example 6000/9728\n",
      "Processing train example 6100/9728\n",
      "Processing train example 6200/9728\n",
      "Processing train example 6300/9728\n",
      "Processing train example 6400/9728\n",
      "Processing train example 6500/9728\n",
      "Processing train example 6600/9728\n",
      "Processing train example 6700/9728\n",
      "Processing train example 6800/9728\n",
      "Processing train example 6900/9728\n",
      "Processing train example 7000/9728\n",
      "Processing train example 7100/9728\n",
      "Processing train example 7200/9728\n",
      "Processing train example 7300/9728\n",
      "Processing train example 7400/9728\n",
      "Processing train example 7500/9728\n",
      "Processing train example 7600/9728\n",
      "Processing train example 7700/9728\n",
      "Processing train example 7800/9728\n",
      "Processing train example 7900/9728\n",
      "Processing train example 8000/9728\n",
      "Processing train example 8100/9728\n",
      "Processing train example 8200/9728\n",
      "Processing train example 8300/9728\n",
      "Processing train example 8400/9728\n",
      "Processing train example 8500/9728\n",
      "Processing train example 8600/9728\n",
      "Processing train example 8700/9728\n",
      "Processing train example 8800/9728\n",
      "Processing train example 8900/9728\n",
      "Processing train example 9000/9728\n",
      "Processing train example 9100/9728\n",
      "Processing train example 9200/9728\n",
      "Processing train example 9300/9728\n",
      "Processing train example 9400/9728\n",
      "Processing train example 9500/9728\n",
      "Processing train example 9600/9728\n",
      "Processing train example 9700/9728\n",
      "Successfully processed 9728/9728 training examples\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "# Step 1: Preprocessing function - DON'T pad here, just truncate\n",
    "def preprocess_example(example):\n",
    "    audio = example[\"input_values\"]\n",
    "    audio = np.array(audio, dtype=np.float32)\n",
    "\n",
    "    max_len = 16000 * 10  # 10 seconds\n",
    "    # Only truncate if too long, don't pad here\n",
    "    if len(audio) > max_len:\n",
    "        audio = audio[:max_len]\n",
    "    \n",
    "    return {\"input_values\": audio}  # Return original length\n",
    "\n",
    "# Step 2: Define memory-safe data collator - padding happens here during batch creation\n",
    "class DataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        input_values = [torch.tensor(f[\"input_values\"], dtype=torch.float32) for f in features]\n",
    "        labels = [f[\"labels\"] for f in features]\n",
    "        max_len = max(len(i) for i in input_values)\n",
    "        padded = [torch.cat([i, torch.zeros(max_len - len(i))]) if len(i) < max_len else i for i in input_values]\n",
    "        batch_inputs = torch.stack(padded)\n",
    "        return {\"input_values\": batch_inputs, \"labels\": torch.tensor(labels, dtype=torch.long)}\n",
    "\n",
    "collator = DataCollator(processor)\n",
    "\n",
    "\n",
    "# %%\n",
    "print(\"Preprocessing train_dataset...\")\n",
    "\n",
    "# Process in smaller chunks and clear cache frequently\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_example,\n",
    "    batched=False,\n",
    "    desc=\"Processing train\",\n",
    "    load_from_cache_file=False,\n",
    "    writer_batch_size=100  # Write to disk more frequently\n",
    ")\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "print(\"âœ… Done with train_dataset\")\n",
    "\n",
    "\n",
    "# %%\n",
    "print(\"Preprocessing valid_dataset...\")\n",
    "\n",
    "valid_dataset = valid_dataset.map(\n",
    "    preprocess_example,\n",
    "    batched=False,\n",
    "    desc=\"Processing valid\",\n",
    "    load_from_cache_file=False,\n",
    "    writer_batch_size=100\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "print(\"âœ… Done with valid_dataset\")\n",
    "\n",
    "print(\"Preprocessing test_dataset...\")\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    preprocess_example,\n",
    "    batched=False,\n",
    "    desc=\"Processing test\",\n",
    "    load_from_cache_file=False,\n",
    "    writer_batch_size=100\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "print(\"âœ… Done with test_dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f51050b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation dataset...\n",
      "Processing valid example 0/1216\n",
      "Processing valid example 50/1216\n",
      "Processing valid example 100/1216\n",
      "Processing valid example 150/1216\n",
      "Processing valid example 200/1216\n",
      "Processing valid example 250/1216\n",
      "Processing valid example 300/1216\n",
      "Processing valid example 350/1216\n",
      "Processing valid example 400/1216\n",
      "Processing valid example 450/1216\n",
      "Processing valid example 500/1216\n",
      "Processing valid example 550/1216\n",
      "Processing valid example 600/1216\n",
      "Processing valid example 650/1216\n",
      "Processing valid example 700/1216\n",
      "Processing valid example 750/1216\n",
      "Processing valid example 800/1216\n",
      "Processing valid example 850/1216\n",
      "Processing valid example 900/1216\n",
      "Processing valid example 950/1216\n",
      "Processing valid example 1000/1216\n",
      "Processing valid example 1050/1216\n",
      "Processing valid example 1100/1216\n",
      "Processing valid example 1150/1216\n",
      "Processing valid example 1200/1216\n",
      "Successfully processed 1216/1216 validation examples\n",
      "Processing test dataset...\n",
      "Processing test example 0/1217\n",
      "Processing test example 50/1217\n",
      "Processing test example 100/1217\n",
      "Processing test example 150/1217\n",
      "Processing test example 200/1217\n",
      "Processing test example 250/1217\n",
      "Processing test example 300/1217\n",
      "Processing test example 350/1217\n",
      "Processing test example 400/1217\n",
      "Processing test example 450/1217\n",
      "Processing test example 500/1217\n",
      "Processing test example 550/1217\n",
      "Processing test example 600/1217\n",
      "Processing test example 650/1217\n",
      "Processing test example 700/1217\n",
      "Processing test example 750/1217\n",
      "Processing test example 800/1217\n",
      "Processing test example 850/1217\n",
      "Processing test example 900/1217\n",
      "Processing test example 950/1217\n",
      "Processing test example 1000/1217\n",
      "Processing test example 1050/1217\n",
      "Processing test example 1100/1217\n",
      "Processing test example 1150/1217\n",
      "Processing test example 1200/1217\n",
      "Successfully processed 1217/1217 test examples\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Metric\n",
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    return {\"accuracy\": (preds == pred.label_ids).mean()}\n",
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-emotion\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    dataloader_num_workers=0,  # Reduce memory overhead\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
